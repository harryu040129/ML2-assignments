{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms , datasets\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Visualize at least one image for each class. You may need to look into how dataset is implemented in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(trainset.targets))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i, j = 0, 0\n",
    "while j < 10 and i < len(trainset.targets):\n",
    "  plt.subplot(2, 5, j + 1)\n",
    "  if trainset.targets[i] == labels[j]:\n",
    "    plt.imshow(trainset.data[i])\n",
    "    plt.title(\"Class {}\".format(labels[j]))\n",
    "    plt.axis('off')\n",
    "    j += 1 \n",
    "  i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Split the trainset into training set and validation set with 90% : 10% ratio. Implement dataloaders for CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 45000\n",
      "Validation Set Size: 5000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def split_train_val(trainset):\n",
    "  num = len(trainset)\n",
    "  num_val = int(num*0.1)\n",
    "  avail_idx = [i for i in range(0,num)]\n",
    "  val_idx = []\n",
    "\n",
    "  for i in range(num_val):\n",
    "    idx = int(torch.rand(1)*len(avail_idx))\n",
    "    # Add selected data to the validation set\n",
    "    temp = avail_idx[idx]\n",
    "    val_idx.append(temp)\n",
    "    avail_idx.pop(idx) \n",
    "\n",
    "  # pass the idx to the Subset\n",
    "  train_set = Subset(trainset, avail_idx)\n",
    "  val_set = Subset(trainset, val_idx)\n",
    "\n",
    "  return train_set, val_set\n",
    "\n",
    "train_set, val_set = split_train_val(trainset)\n",
    "\n",
    "print(f\"Training Set Size: {len(train_set)}\")\n",
    "print(f\"Validation Set Size: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Choose any two clases and implement a linear SVM model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM(\n",
      "  (layer1): Linear(in_features=3072, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the SVM Model\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# can not have more than 1 layer\n",
    "class SVM(nn.Module):\n",
    "  def __init__(self, input_dim, num_classes):\n",
    "    super(SVM, self).__init__()\n",
    "    self.layer1 = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.layer1(x)\n",
    "\n",
    "    return x\n",
    "model = SVM(32*32*3, num_classes=1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optimizer: SGD\n",
    "from torch.optim import SGD\n",
    "\n",
    "def optimizer(model, lr):\n",
    "  return SGD(model.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include class 0 and 1 only\n",
    "def filter_0_1(dataset):\n",
    "  # Get indices where labels are 0 or 1\n",
    "  idx = []\n",
    "  for i in range(len(dataset)):\n",
    "    x, label = dataset[i]\n",
    "    if label in [0, 1]:\n",
    "      idx.append(i)\n",
    "\n",
    "  # Create filtered dataset\n",
    "  filtered = Subset(dataset, idx)\n",
    "  return DataLoader(filtered, batch_size=64, shuffle=True)\n",
    "\n",
    "# First filter the datasets\n",
    "train_set = filter_0_1(train_set)\n",
    "val_set = filter_0_1(val_set)\n",
    "testset = filter_0_1(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss Function: Hinge Loss\n",
    "# Create accuracy function\n",
    "\n",
    "def loss_hinge(y_pred, y_true, gamma, model):\n",
    "  # if y_true is 0, convert to -1\n",
    "  # if y_true is 1, convert to 1\n",
    "  y_true = 2 * y_true.float() - 1\n",
    "  # Ensure dimensions match\n",
    "  y_true = y_true.unsqueeze(1)\n",
    "  \n",
    "  correct = correct_pred(y_pred, y_true)\n",
    "  losses = torch.max(torch.zeros(y_pred.shape), 1 - y_pred * y_true)\n",
    "  losses += gamma * torch.sum(model.layer1.weight ** 2)\n",
    "  \n",
    "  return losses.mean(), correct\n",
    "\n",
    "# Counts the number of correct predictions\n",
    "def correct_pred(y_pred, y_true):\n",
    "  conv_pred = []\n",
    "  correct = 0\n",
    "  for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 0:\n",
    "      conv_pred.append(1)\n",
    "    else:\n",
    "      conv_pred.append(-1)\n",
    "  y_true = list(y_true)\n",
    "  for i in range(len(y_true)):\n",
    "    if y_true[i] == conv_pred[i]:\n",
    "      correct += 1\n",
    "        \n",
    "  return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_loop(dataloader, model, gamma, lr):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  \n",
    "  # Define the optimizer\n",
    "  optim = optimizer(model, lr)\n",
    "    \n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    pred = model(X)\n",
    "    loss, correct = loss_hinge(pred, y, gamma, model)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    total_loss += loss.item() * X.size(0)\n",
    "    total_correct += correct\n",
    "  print(f\"Training loss: {total_loss / size}\")\n",
    "  print(f\"Training accuracy: {total_correct / size}\")\n",
    "  return total_correct / size\n",
    "  \n",
    "# Test the model\n",
    "def test_loop(dataloader, model, gamma):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      pred = model(X)\n",
    "      loss, correct = loss_hinge(pred, y, gamma, model)\n",
    "\n",
    "      total_loss += loss.item() * X.size(0)\n",
    "      total_correct += correct\n",
    "      \n",
    "  print(f\"Test loss: {total_loss / size}\")\n",
    "  print(f\"Test accuracy: {total_correct / size}\")\n",
    "  return total_correct / size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Train for 10 epochs with batch size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.9502927305169139\n",
      "Training accuracy: 0.4984887495802082\n",
      "Test loss: 0.9193738679885864\n",
      "Test accuracy: 0.5005\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.915363898572737\n",
      "Training accuracy: 0.49938430538452927\n",
      "Test loss: 0.8883646993637085\n",
      "Test accuracy: 0.5015\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.8871518860708809\n",
      "Training accuracy: 0.5024068062241128\n",
      "Test loss: 0.8605524883270264\n",
      "Test accuracy: 0.5125\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.8606322360014736\n",
      "Training accuracy: 0.5174073659464905\n",
      "Test loss: 0.8335201873779297\n",
      "Test accuracy: 0.532\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.835769588731377\n",
      "Training accuracy: 0.5485279301466472\n",
      "Test loss: 0.8086364259719848\n",
      "Test accuracy: 0.565\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.8118459351930455\n",
      "Training accuracy: 0.5868129407813725\n",
      "Test loss: 0.78656804895401\n",
      "Test accuracy: 0.6415\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.7893112130011901\n",
      "Training accuracy: 0.6371879547744319\n",
      "Test loss: 0.7620335917472839\n",
      "Test accuracy: 0.6675\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.7670739346344287\n",
      "Training accuracy: 0.6726743535206537\n",
      "Test loss: 0.7402015423774719\n",
      "Test accuracy: 0.7105\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.745656639747893\n",
      "Training accuracy: 0.7103996417776782\n",
      "Test loss: 0.7180518651008606\n",
      "Test accuracy: 0.723\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.7252681879010804\n",
      "Training accuracy: 0.7293182581439606\n",
      "Test loss: 0.6978337035179139\n",
      "Test accuracy: 0.742\n"
     ]
    }
   ],
   "source": [
    "# Train for 10 epochs\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  train_acc = train_loop(train_set, model, gamma=0.01, lr = 0.0001)\n",
    "  test_acc = test_loop(testset, model, gamma=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Perform data normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4914, 0.4822, 0.4465])\n",
      "Std: tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "# create Tensor to store mean and std\n",
    "mean = torch.zeros(3)\n",
    "std = torch.zeros(3)\n",
    "\n",
    "# Calculate mean and std across all training images\n",
    "# index 0: red, 1: green, 2: blue\n",
    "# 4th dimension is the channel\n",
    "for i in range(3):\n",
    "  mean[i] = trainset.data[:, :, :, i].mean()\n",
    "  std[i] = trainset.data[:, :, :, i].std()\n",
    "\n",
    "# divide by 255 to scale to [0,1]\n",
    "print('Mean:', mean/255)\n",
    "print('Std:', std/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Create transform for Normalization\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Apply transform to the dataset and re-do the steps for data loading\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_set, val_set = split_train_val(trainset)\n",
    "\n",
    "train_set = filter_0_1(train_set)\n",
    "val_set = filter_0_1(val_set)\n",
    "testset = filter_0_1(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) train for 10 epochs after data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.7341207106465417\n",
      "Training accuracy: 0.6756545961002786\n",
      "Test loss: 0.6010925483703613\n",
      "Test accuracy: 0.753\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.5826823095724111\n",
      "Training accuracy: 0.757883008356546\n",
      "Test loss: 0.5477042317390441\n",
      "Test accuracy: 0.776\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.5472363972132585\n",
      "Training accuracy: 0.7716991643454039\n",
      "Test loss: 0.5262213597297668\n",
      "Test accuracy: 0.7815\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.5290339574906819\n",
      "Training accuracy: 0.7786072423398329\n",
      "Test loss: 0.5123192462921142\n",
      "Test accuracy: 0.7825\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.5169282875154012\n",
      "Training accuracy: 0.7844011142061281\n",
      "Test loss: 0.5025943906307221\n",
      "Test accuracy: 0.791\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.5076101459301282\n",
      "Training accuracy: 0.787966573816156\n",
      "Test loss: 0.49486645340919494\n",
      "Test accuracy: 0.7995\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.5004181391083763\n",
      "Training accuracy: 0.7914206128133705\n",
      "Test loss: 0.4888177032470703\n",
      "Test accuracy: 0.798\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.4939483147942588\n",
      "Training accuracy: 0.7956545961002786\n",
      "Test loss: 0.4831104679107666\n",
      "Test accuracy: 0.798\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.4889203635663375\n",
      "Training accuracy: 0.796100278551532\n",
      "Test loss: 0.478832967042923\n",
      "Test accuracy: 0.8015\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.48433364928432826\n",
      "Training accuracy: 0.7978830083565459\n",
      "Test loss: 0.4749788610935211\n",
      "Test accuracy: 0.8065\n"
     ]
    }
   ],
   "source": [
    "# train for 10 epochs\n",
    "model_norm = SVM(32*32*3, num_classes=1)\n",
    "model_norm = model_norm.to(device)\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  train_acc = train_loop(train_set, model_norm, gamma=0.01, lr = 0.0001)\n",
    "  test_acc = test_loop(testset, model_norm, gamma=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) hyperparameter tuning: gamma, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5466857616539827\n",
      "Training accuracy: 0.7695134172141187\n",
      "Test loss: 0.5443476135527187\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 2, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4719552381849544\n",
      "Training accuracy: 0.8083732323794678\n",
      "Test loss: 0.5231851118528106\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 3, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.455566577265754\n",
      "Training accuracy: 0.8159447722970716\n",
      "Test loss: 0.5027584929578555\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 4, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4460603509962313\n",
      "Training accuracy: 0.8182830419775081\n",
      "Test loss: 0.5011880134869371\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4372372500031818\n",
      "Training accuracy: 0.8214007348847567\n",
      "Test loss: 0.5016560639319639\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 6, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4332503894431702\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.49664746004068117\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.43016227770191484\n",
      "Training accuracy: 0.8258545818951119\n",
      "Test loss: 0.4980717505217768\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 8, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4268048212716978\n",
      "Training accuracy: 0.8271907359982185\n",
      "Test loss: 0.49199751835459\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 9, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4239860946239335\n",
      "Training accuracy: 0.8288609286271017\n",
      "Test loss: 0.5044289173684481\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 10, lr: 0.001, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4213081802892839\n",
      "Training accuracy: 0.829529005678655\n",
      "Test loss: 0.49247891792956233\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 1, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5203675465970856\n",
      "Training accuracy: 0.7820955350183721\n",
      "Test loss: 0.559590969340724\n",
      "Test accuracy: 0.746810598626104\n",
      "Epoch 2, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.45596114877815064\n",
      "Training accuracy: 0.8129384255650819\n",
      "Test loss: 0.527465021645356\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 3, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4445110943731281\n",
      "Training accuracy: 0.8193965037300969\n",
      "Test loss: 0.5022022149162741\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 4, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4362153807722796\n",
      "Training accuracy: 0.8219574657610511\n",
      "Test loss: 0.49590710954881395\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 5, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4313296006299541\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.4914142917604044\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 6, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4246687112184665\n",
      "Training accuracy: 0.8281928515755483\n",
      "Test loss: 0.5169601152642787\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 7, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4238985553110061\n",
      "Training accuracy: 0.8273020821734773\n",
      "Test loss: 0.5133719572667635\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 8, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.41766842444467855\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.4966862255796951\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 9, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4171241249463139\n",
      "Training accuracy: 0.8334261218127157\n",
      "Test loss: 0.4900682764502573\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 10, lr: 0.002, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.41461908578700124\n",
      "Training accuracy: 0.8329807371116802\n",
      "Test loss: 0.5205532966640443\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 1, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5144302154181994\n",
      "Training accuracy: 0.7847678432245853\n",
      "Test loss: 0.5136776985317264\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 2, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4573804043924446\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.5016552273611793\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4434059315037852\n",
      "Training accuracy: 0.8219574657610511\n",
      "Test loss: 0.5110399305995945\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 4, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4363114121509493\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.6191312766051736\n",
      "Test accuracy: 0.7212953876349362\n",
      "Epoch 5, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4299953480550513\n",
      "Training accuracy: 0.8299743903796905\n",
      "Test loss: 0.6149555407281713\n",
      "Test accuracy: 0.7625122669283612\n",
      "Epoch 6, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4264501424657915\n",
      "Training accuracy: 0.8277474668745128\n",
      "Test loss: 0.5403353081425694\n",
      "Test accuracy: 0.760549558390579\n",
      "Epoch 7, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.42314237364150015\n",
      "Training accuracy: 0.8300857365549493\n",
      "Test loss: 0.5662835584596984\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 8, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.41910717891446864\n",
      "Training accuracy: 0.8342055450395279\n",
      "Test loss: 0.5009309073893666\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 9, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.41703425620409496\n",
      "Training accuracy: 0.8323126600601269\n",
      "Test loss: 0.5655330434163723\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 10, lr: 0.003, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4178884385649126\n",
      "Training accuracy: 0.8326466985859036\n",
      "Test loss: 0.529759627621588\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 1, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.506735768829936\n",
      "Training accuracy: 0.7930074601937424\n",
      "Test loss: 0.5381316451375922\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 2, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4586687939640356\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.5355712944263331\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 3, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4541838976458443\n",
      "Training accuracy: 0.8163901569981071\n",
      "Test loss: 0.5312844183189946\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 4, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.44058872893902284\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.4994770173820593\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 5, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.43902947571240186\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.5101218878111498\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 6, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.43229786798153885\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.5432182014631921\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 7, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.42768550213800083\n",
      "Training accuracy: 0.8313105444827971\n",
      "Test loss: 0.5929395911504055\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 8, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4285361242478813\n",
      "Training accuracy: 0.8287495824518428\n",
      "Test loss: 0.5285240116367397\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 9, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.42802056897497565\n",
      "Training accuracy: 0.8300857365549493\n",
      "Test loss: 0.54223040854732\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 10, lr: 0.004, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4208213186391132\n",
      "Training accuracy: 0.8336488141632334\n",
      "Test loss: 0.5362024215709005\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 1, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5209636777032126\n",
      "Training accuracy: 0.7869947667297629\n",
      "Test loss: 0.6756632877577743\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 2, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4689619624160762\n",
      "Training accuracy: 0.8134951564413763\n",
      "Test loss: 0.5281408212720704\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 3, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.46189891662527044\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.564980892777092\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 4, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4488477088629622\n",
      "Training accuracy: 0.8232936198641576\n",
      "Test loss: 0.5740262534948279\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 5, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.44357782632839227\n",
      "Training accuracy: 0.8218461195857922\n",
      "Test loss: 0.5438708626720926\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 6, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4441683285221227\n",
      "Training accuracy: 0.8218461195857922\n",
      "Test loss: 0.5034083015083448\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 7, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.43698728947673443\n",
      "Training accuracy: 0.828638236276584\n",
      "Test loss: 0.5116578427100439\n",
      "Test accuracy: 0.8007850834151129\n",
      "Epoch 8, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4334967441938991\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.6467185504771075\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 9, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.44306656662505395\n",
      "Training accuracy: 0.8246297739672642\n",
      "Test loss: 0.5076599677364773\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 10, lr: 0.005, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.42522809950390317\n",
      "Training accuracy: 0.8308651597817615\n",
      "Test loss: 0.6861931386826902\n",
      "Test accuracy: 0.7055937193326791\n",
      "Epoch 1, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5338483833215402\n",
      "Training accuracy: 0.7804253423894889\n",
      "Test loss: 0.7500500824079429\n",
      "Test accuracy: 0.7340529931305201\n",
      "Epoch 2, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.48427584794220446\n",
      "Training accuracy: 0.8069257321011023\n",
      "Test loss: 0.57855277247471\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 3, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.46764631343570134\n",
      "Training accuracy: 0.8147199643692239\n",
      "Test loss: 0.5767939838272783\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 4, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.472076257968079\n",
      "Training accuracy: 0.8136065026166351\n",
      "Test loss: 0.6994057451543911\n",
      "Test accuracy: 0.7144259077526988\n",
      "Epoch 5, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4590971062780953\n",
      "Training accuracy: 0.8202872731321679\n",
      "Test loss: 0.5417360974598212\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 6, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.45394548037584154\n",
      "Training accuracy: 0.8206213116579445\n",
      "Test loss: 0.5375957576461114\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 7, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4538143574627374\n",
      "Training accuracy: 0.8221801581115689\n",
      "Test loss: 0.5951824358447845\n",
      "Test accuracy: 0.7477919528949951\n",
      "Epoch 8, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.44596681869709603\n",
      "Training accuracy: 0.828526890101325\n",
      "Test loss: 0.5098413756597966\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 9, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4423825626794857\n",
      "Training accuracy: 0.8254091971940763\n",
      "Test loss: 0.5520221133870863\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 10, lr: 0.006, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.45594377538141384\n",
      "Training accuracy: 0.823961696915711\n",
      "Test loss: 0.6770588729870565\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 1, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5476003337206414\n",
      "Training accuracy: 0.7792005344616413\n",
      "Test loss: 0.7073260078954275\n",
      "Test accuracy: 0.6918547595682041\n",
      "Epoch 2, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5081327473940741\n",
      "Training accuracy: 0.8039193853691126\n",
      "Test loss: 0.611211055218881\n",
      "Test accuracy: 0.7419038272816487\n",
      "Epoch 3, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.487597452354463\n",
      "Training accuracy: 0.8134951564413763\n",
      "Test loss: 0.5889172178489304\n",
      "Test accuracy: 0.760549558390579\n",
      "Epoch 4, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.48691780507969384\n",
      "Training accuracy: 0.8091526556062799\n",
      "Test loss: 0.7048394713949292\n",
      "Test accuracy: 0.7212953876349362\n",
      "Epoch 5, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4710413263935433\n",
      "Training accuracy: 0.8163901569981071\n",
      "Test loss: 0.5448044563650032\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 6, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4739185854694703\n",
      "Training accuracy: 0.8172809264001781\n",
      "Test loss: 0.737958058407777\n",
      "Test accuracy: 0.7026496565260059\n",
      "Epoch 7, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4678215275541099\n",
      "Training accuracy: 0.8210666963589801\n",
      "Test loss: 0.6981664037915044\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 8, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4649980673743786\n",
      "Training accuracy: 0.8196191960806146\n",
      "Test loss: 0.5480986630015799\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 9, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4531040998214524\n",
      "Training accuracy: 0.8245184277920053\n",
      "Test loss: 0.7392818360087682\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 10, lr: 0.007, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4635617617455905\n",
      "Training accuracy: 0.822959581338381\n",
      "Test loss: 0.7225622895655384\n",
      "Test accuracy: 0.7124631992149166\n",
      "Epoch 1, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5707288487406725\n",
      "Training accuracy: 0.7781984188843113\n",
      "Test loss: 1.2212867330873094\n",
      "Test accuracy: 0.577036310107949\n",
      "Epoch 2, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5259220086244681\n",
      "Training accuracy: 0.7976839995546153\n",
      "Test loss: 0.7691229884828038\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 3, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5039428779203102\n",
      "Training accuracy: 0.8087072709052444\n",
      "Test loss: 0.8737487054198717\n",
      "Test accuracy: 0.6575073601570167\n",
      "Epoch 4, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4978754790399218\n",
      "Training accuracy: 0.8091526556062799\n",
      "Test loss: 0.6244308229634525\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 5, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5005918579156781\n",
      "Training accuracy: 0.8077051553279145\n",
      "Test loss: 0.6605205166796122\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 6, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.49542542601491835\n",
      "Training accuracy: 0.8141632334929295\n",
      "Test loss: 0.5726576944094762\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 7, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.47496686215297695\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.6120076460272103\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 8, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.48917469306958966\n",
      "Training accuracy: 0.8169468878744015\n",
      "Test loss: 0.6824821210352081\n",
      "Test accuracy: 0.732090284592738\n",
      "Epoch 9, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4764573438278273\n",
      "Training accuracy: 0.8171695802249193\n",
      "Test loss: 0.5719339743446672\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 10, lr: 0.008, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.47712420034801467\n",
      "Training accuracy: 0.8193965037300969\n",
      "Test loss: 0.8700506846266008\n",
      "Test accuracy: 0.6604514229636899\n",
      "Epoch 1, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5833230795845424\n",
      "Training accuracy: 0.776639572430687\n",
      "Test loss: 0.5574225590905216\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 2, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.538414506737154\n",
      "Training accuracy: 0.7965705378020265\n",
      "Test loss: 0.5403797908432936\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 3, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5284608535492636\n",
      "Training accuracy: 0.8034740006680771\n",
      "Test loss: 0.5493481407221681\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 4, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5205052271742503\n",
      "Training accuracy: 0.8042534238948892\n",
      "Test loss: 0.771904030597245\n",
      "Test accuracy: 0.7497546614327772\n",
      "Epoch 5, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5158756054525532\n",
      "Training accuracy: 0.8101547711836098\n",
      "Test loss: 0.7360906882889723\n",
      "Test accuracy: 0.6987242394504416\n",
      "Epoch 6, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5163189957308061\n",
      "Training accuracy: 0.8104888097093865\n",
      "Test loss: 0.5652095345273921\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 7, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4923023759402885\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.5369327881385346\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 8, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.496073900627912\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.978022056952543\n",
      "Test accuracy: 0.7350343473994112\n",
      "Epoch 9, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.5058826756476028\n",
      "Training accuracy: 0.8149426567197416\n",
      "Test loss: 0.6946860873547106\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 10, lr: 0.009, gamma: 0.01\n",
      "-------------------------------\n",
      "Training loss: 0.4935960805010602\n",
      "Training accuracy: 0.815610733771295\n",
      "Test loss: 0.5881467208310128\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 1, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5404284684214185\n",
      "Training accuracy: 0.7789778421111235\n",
      "Test loss: 0.5418516392796735\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 2, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4744859590301826\n",
      "Training accuracy: 0.8072597706268789\n",
      "Test loss: 0.5221699402540542\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 3, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4577586319873705\n",
      "Training accuracy: 0.8158334261218128\n",
      "Test loss: 0.4973540001048892\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4494133462434409\n",
      "Training accuracy: 0.8198418884311324\n",
      "Test loss: 0.49542537410547033\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44304222651319586\n",
      "Training accuracy: 0.8218461195857922\n",
      "Test loss: 0.49747413589863126\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 6, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43769628837225155\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.5182891033290062\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 7, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4343178208003582\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.5190498506590008\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 8, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42973110125227054\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.5206158442223505\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 9, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4278233279471487\n",
      "Training accuracy: 0.8264113127714063\n",
      "Test loss: 0.49169118136489237\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 10, lr: 0.001, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42566326599704596\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.5070285794194477\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 1, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5235661872643005\n",
      "Training accuracy: 0.7834316891214786\n",
      "Test loss: 0.5197022098086416\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 2, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.46472695357363364\n",
      "Training accuracy: 0.8113795791114575\n",
      "Test loss: 0.5146794573891034\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 3, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4497919420717607\n",
      "Training accuracy: 0.8199532346063912\n",
      "Test loss: 0.5071766904689613\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 4, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44284375076463317\n",
      "Training accuracy: 0.8215120810600156\n",
      "Test loss: 0.49869266085114633\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 5, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4362267322416043\n",
      "Training accuracy: 0.8269680436477007\n",
      "Test loss: 0.6995683710755263\n",
      "Test accuracy: 0.6957801766437685\n",
      "Epoch 6, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4343715003014354\n",
      "Training accuracy: 0.8246297739672642\n",
      "Test loss: 0.5020369387234509\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 7, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42658618291307565\n",
      "Training accuracy: 0.828526890101325\n",
      "Test loss: 0.5136087188191923\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 8, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42428342969655697\n",
      "Training accuracy: 0.8310878521322792\n",
      "Test loss: 0.5516135999686117\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 9, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4215073219891739\n",
      "Training accuracy: 0.8323126600601269\n",
      "Test loss: 0.4967573000825539\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 10, lr: 0.002, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42186075295017283\n",
      "Training accuracy: 0.8300857365549493\n",
      "Test loss: 0.5055152639615056\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 1, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5133436778521913\n",
      "Training accuracy: 0.7920053446164125\n",
      "Test loss: 0.5126744684808037\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 2, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45761565502881074\n",
      "Training accuracy: 0.8113795791114575\n",
      "Test loss: 0.5125276317657269\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4499215720896067\n",
      "Training accuracy: 0.8210666963589801\n",
      "Test loss: 0.5171538137184626\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 4, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4403611571028084\n",
      "Training accuracy: 0.8254091971940763\n",
      "Test loss: 0.4974017762108336\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 5, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43421472833863\n",
      "Training accuracy: 0.8293063133281372\n",
      "Test loss: 0.4953595488181409\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 6, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43193824009698345\n",
      "Training accuracy: 0.8294176595033961\n",
      "Test loss: 0.5109911377928325\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 7, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43104593706162836\n",
      "Training accuracy: 0.8307538136065026\n",
      "Test loss: 0.5518026236112031\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 8, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42788033218913935\n",
      "Training accuracy: 0.8313105444827971\n",
      "Test loss: 0.6157546327088368\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 9, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42574506440907034\n",
      "Training accuracy: 0.8314218906580559\n",
      "Test loss: 0.5426217662106553\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 10, lr: 0.003, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42189985151320625\n",
      "Training accuracy: 0.8336488141632334\n",
      "Test loss: 0.5249603529907654\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 1, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5187667968603408\n",
      "Training accuracy: 0.7872174590802806\n",
      "Test loss: 0.675568108469745\n",
      "Test accuracy: 0.7154072620215898\n",
      "Epoch 2, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.46784861481225565\n",
      "Training accuracy: 0.8121590023382697\n",
      "Test loss: 0.547812898446814\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 3, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45989817478133527\n",
      "Training accuracy: 0.8149426567197416\n",
      "Test loss: 0.5115176505382676\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 4, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44690496885568126\n",
      "Training accuracy: 0.8212893887094979\n",
      "Test loss: 0.6196980479304058\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 5, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4415026002976985\n",
      "Training accuracy: 0.8254091971940763\n",
      "Test loss: 0.5147637427145646\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 6, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4411776388239693\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.5650231824187933\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 7, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43745638645315155\n",
      "Training accuracy: 0.8240730430909698\n",
      "Test loss: 0.5013403683988573\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 8, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.42700763641228295\n",
      "Training accuracy: 0.8327580447611624\n",
      "Test loss: 0.5266094116806633\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 9, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4299700767077259\n",
      "Training accuracy: 0.8288609286271017\n",
      "Test loss: 0.5026332972328141\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 10, lr: 0.004, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4288760655415561\n",
      "Training accuracy: 0.8320899677096092\n",
      "Test loss: 0.5809211708144655\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 1, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5363843484546597\n",
      "Training accuracy: 0.7833203429462198\n",
      "Test loss: 0.7924752675282007\n",
      "Test accuracy: 0.7340529931305201\n",
      "Epoch 2, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.48582517192167557\n",
      "Training accuracy: 0.8032513083175593\n",
      "Test loss: 0.6284025119027165\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 3, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45971574315168906\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5090223224404048\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 4, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4575854073639583\n",
      "Training accuracy: 0.8166128493486249\n",
      "Test loss: 0.6000023223057579\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 5, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4592493263136211\n",
      "Training accuracy: 0.8207326578332035\n",
      "Test loss: 0.49967243071276257\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 6, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45308005192898093\n",
      "Training accuracy: 0.8235163122146755\n",
      "Test loss: 0.6128158528390647\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 7, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4438824182349926\n",
      "Training accuracy: 0.8269680436477007\n",
      "Test loss: 0.545817002580959\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 8, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44332988398760814\n",
      "Training accuracy: 0.8261886204208886\n",
      "Test loss: 0.5727468273356572\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 9, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44660925492188036\n",
      "Training accuracy: 0.8247411201425231\n",
      "Test loss: 0.5043779215003136\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 10, lr: 0.005, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.43208226279484485\n",
      "Training accuracy: 0.8311991983075382\n",
      "Test loss: 0.5919723973423974\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 1, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5362541318444152\n",
      "Training accuracy: 0.7861039973276918\n",
      "Test loss: 0.5802778809297541\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 2, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4926695607377776\n",
      "Training accuracy: 0.8074824629773967\n",
      "Test loss: 0.5926448290434623\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 3, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4696690763354421\n",
      "Training accuracy: 0.8161674646475894\n",
      "Test loss: 0.6413021620520197\n",
      "Test accuracy: 0.7438665358194309\n",
      "Epoch 4, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4577830628501375\n",
      "Training accuracy: 0.8206213116579445\n",
      "Test loss: 0.5298589852888053\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 5, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4624831754863163\n",
      "Training accuracy: 0.8217347734105334\n",
      "Test loss: 0.5449724292146797\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 6, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4519640053351251\n",
      "Training accuracy: 0.8234049660394166\n",
      "Test loss: 0.5604108591309006\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 7, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45957638662279005\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.8202749653349213\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 8, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.44889883140375586\n",
      "Training accuracy: 0.8287495824518428\n",
      "Test loss: 0.6112726311454361\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 9, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.45066905216787906\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.925069868623099\n",
      "Test accuracy: 0.6555446516192346\n",
      "Epoch 10, lr: 0.006, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.469737838934238\n",
      "Training accuracy: 0.8216234272352745\n",
      "Test loss: 0.5654378958024502\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 1, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5569326340780533\n",
      "Training accuracy: 0.782206881193631\n",
      "Test loss: 0.561471076018677\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 2, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5056051941448112\n",
      "Training accuracy: 0.8025832312660061\n",
      "Test loss: 0.5297352355759558\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 3, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4920172767103707\n",
      "Training accuracy: 0.8130497717403408\n",
      "Test loss: 0.535295894733238\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 4, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4904432542578121\n",
      "Training accuracy: 0.8134951564413763\n",
      "Test loss: 0.6828588599428695\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 5, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4748061615233299\n",
      "Training accuracy: 0.8191738113795791\n",
      "Test loss: 0.9462932479276274\n",
      "Test accuracy: 0.6467124631992149\n",
      "Epoch 6, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4761080679521769\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5400006676328545\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 7, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4707659424818009\n",
      "Training accuracy: 0.8208440040084624\n",
      "Test loss: 0.6838397796067452\n",
      "Test accuracy: 0.7477919528949951\n",
      "Epoch 8, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.46252005596811824\n",
      "Training accuracy: 0.8234049660394166\n",
      "Test loss: 0.6166432839731473\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 9, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.47445770620622796\n",
      "Training accuracy: 0.8227368889878632\n",
      "Test loss: 0.5619976379569543\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 10, lr: 0.007, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.47331499775989644\n",
      "Training accuracy: 0.8221801581115689\n",
      "Test loss: 0.5280399572568506\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 1, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5646804450429265\n",
      "Training accuracy: 0.7833203429462198\n",
      "Test loss: 0.5405153212182304\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 2, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.524273889478391\n",
      "Training accuracy: 0.7982407304309097\n",
      "Test loss: 0.5391198677450447\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 3, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5063144429305284\n",
      "Training accuracy: 0.8073711168021378\n",
      "Test loss: 0.5308523105568927\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 4, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4988774831630037\n",
      "Training accuracy: 0.8108228482351632\n",
      "Test loss: 0.6128226726282099\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 5, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4945810850970584\n",
      "Training accuracy: 0.8140518873176706\n",
      "Test loss: 1.0472393085607954\n",
      "Test accuracy: 0.7360157016683022\n",
      "Epoch 6, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.4973940421134323\n",
      "Training accuracy: 0.8120476561630108\n",
      "Test loss: 0.653085538870687\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 7, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5081653983718288\n",
      "Training accuracy: 0.8088186170805033\n",
      "Test loss: 0.718726682592772\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 8, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.48005422743388054\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5969947154786331\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 9, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.48286007417892907\n",
      "Training accuracy: 0.8160561184723305\n",
      "Test loss: 0.6221141963173527\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 10, lr: 0.008, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.47864665672914736\n",
      "Training accuracy: 0.8203986193074267\n",
      "Test loss: 0.8994367887742152\n",
      "Test accuracy: 0.7438665358194309\n",
      "Epoch 1, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.6197743579362021\n",
      "Training accuracy: 0.7689566863378243\n",
      "Test loss: 1.1928045973108607\n",
      "Test accuracy: 0.5789990186457311\n",
      "Epoch 2, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5519836287683773\n",
      "Training accuracy: 0.7934528448947779\n",
      "Test loss: 0.6696176162762778\n",
      "Test accuracy: 0.7369970559371933\n",
      "Epoch 3, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5202740698558372\n",
      "Training accuracy: 0.8067030397505845\n",
      "Test loss: 0.7542156457199082\n",
      "Test accuracy: 0.703631010794897\n",
      "Epoch 4, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5493016970567774\n",
      "Training accuracy: 0.7984634227814275\n",
      "Test loss: 0.6349173337952312\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 5, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5111203109045999\n",
      "Training accuracy: 0.8083732323794678\n",
      "Test loss: 0.7228131283721699\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 6, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5100756692964002\n",
      "Training accuracy: 0.8154993875960361\n",
      "Test loss: 0.6740039122584289\n",
      "Test accuracy: 0.760549558390579\n",
      "Epoch 7, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5210194307835805\n",
      "Training accuracy: 0.8110455405856809\n",
      "Test loss: 0.6686403345429979\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 8, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5114760695195599\n",
      "Training accuracy: 0.8088186170805033\n",
      "Test loss: 0.7575529385595724\n",
      "Test accuracy: 0.7124631992149166\n",
      "Epoch 9, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.5035211104962279\n",
      "Training accuracy: 0.8163901569981071\n",
      "Test loss: 0.5479210978986236\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 10, lr: 0.009, gamma: 0.02\n",
      "-------------------------------\n",
      "Training loss: 0.49313371520094984\n",
      "Training accuracy: 0.8197305422558735\n",
      "Test loss: 0.609252515964583\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 1, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5403138422983259\n",
      "Training accuracy: 0.7728538024718851\n",
      "Test loss: 0.5178635457898029\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 2, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.47698695974418465\n",
      "Training accuracy: 0.8050328471217014\n",
      "Test loss: 0.5185624396426638\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4612549738467981\n",
      "Training accuracy: 0.8141632334929295\n",
      "Test loss: 0.4942878957644061\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 4, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.45153171968890143\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.4930924720572302\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 5, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44623604473923273\n",
      "Training accuracy: 0.8187284266785436\n",
      "Test loss: 0.5101865543175025\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 6, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44226379226820595\n",
      "Training accuracy: 0.8256318895445941\n",
      "Test loss: 0.49190257532088866\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43744251384539723\n",
      "Training accuracy: 0.8271907359982185\n",
      "Test loss: 0.48586491888006955\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 8, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43320465530561797\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.48264744260711223\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 9, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43135082166214295\n",
      "Training accuracy: 0.8273020821734773\n",
      "Test loss: 0.48291985855837216\n",
      "Test accuracy: 0.7988223748773308\n",
      "Epoch 10, lr: 0.001, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4288684208891384\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.48337170891134734\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 1, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5168632414549101\n",
      "Training accuracy: 0.7892216902349404\n",
      "Test loss: 0.5165115569126851\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 2, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4621806193262176\n",
      "Training accuracy: 0.8159447722970716\n",
      "Test loss: 0.5791805169533233\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 3, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4486114549293057\n",
      "Training accuracy: 0.8224028504620866\n",
      "Test loss: 0.5630527979843282\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 4, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44208395714557275\n",
      "Training accuracy: 0.8250751586682997\n",
      "Test loss: 0.5191303922395359\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 5, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4381589263614548\n",
      "Training accuracy: 0.8264113127714063\n",
      "Test loss: 0.509392667506932\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 6, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43432110847336797\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5025854479810323\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 7, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43017922168106665\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.505899374571586\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 8, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.42686251148642895\n",
      "Training accuracy: 0.8298630442044316\n",
      "Test loss: 0.5254028023049688\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 9, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.42624588396166685\n",
      "Training accuracy: 0.8334261218127157\n",
      "Test loss: 0.5091334705965793\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 10, lr: 0.002, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.42307914849737815\n",
      "Training accuracy: 0.8337601603384923\n",
      "Test loss: 0.5020989611876491\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 1, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5141731871619892\n",
      "Training accuracy: 0.7906691905133059\n",
      "Test loss: 0.5051470678500736\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 2, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.46597777908488935\n",
      "Training accuracy: 0.8149426567197416\n",
      "Test loss: 0.5385202027864615\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 3, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.45716157066451724\n",
      "Training accuracy: 0.8150540028950005\n",
      "Test loss: 0.5086043667746479\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44303932580580463\n",
      "Training accuracy: 0.8236276583899343\n",
      "Test loss: 0.5119642766815874\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 5, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4405922241398625\n",
      "Training accuracy: 0.8227368889878632\n",
      "Test loss: 0.5226535094030004\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 6, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4355815569292051\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.5130295564545734\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 7, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4342671629249247\n",
      "Training accuracy: 0.8260772742456297\n",
      "Test loss: 0.5449423289275613\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 8, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43246283299300947\n",
      "Training accuracy: 0.828638236276584\n",
      "Test loss: 0.5363045128919659\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 9, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43039078441551276\n",
      "Training accuracy: 0.829529005678655\n",
      "Test loss: 0.5102343468015631\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 10, lr: 0.003, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.42971668756694387\n",
      "Training accuracy: 0.8293063133281372\n",
      "Test loss: 0.523518091212077\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 1, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5096780056976526\n",
      "Training accuracy: 0.7930074601937424\n",
      "Test loss: 0.5625430085532681\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 2, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.46834857725191853\n",
      "Training accuracy: 0.8161674646475894\n",
      "Test loss: 0.5361839359712086\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 3, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4608759531503849\n",
      "Training accuracy: 0.8175036187506959\n",
      "Test loss: 1.116486880624844\n",
      "Test accuracy: 0.5799803729146222\n",
      "Epoch 4, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4585280804157045\n",
      "Training accuracy: 0.8166128493486249\n",
      "Test loss: 0.5352422129417659\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 5, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44490098949928697\n",
      "Training accuracy: 0.8226255428126044\n",
      "Test loss: 0.514181948211172\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 6, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4460773004241542\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.5188193128200228\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 7, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43888439511007754\n",
      "Training accuracy: 0.8288609286271017\n",
      "Test loss: 0.5655841998777866\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 8, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43570048587251353\n",
      "Training accuracy: 0.8293063133281372\n",
      "Test loss: 0.5613363769501302\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 9, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.43599133585890687\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.5573460035632941\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 10, lr: 0.004, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4287487325590287\n",
      "Training accuracy: 0.8352076606168578\n",
      "Test loss: 0.5223497118987326\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 1, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5259609762510761\n",
      "Training accuracy: 0.7913372675648591\n",
      "Test loss: 0.5196261218473885\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 2, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.47582319689784547\n",
      "Training accuracy: 0.8122703485135285\n",
      "Test loss: 0.5462488529726615\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 3, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.46814440881843594\n",
      "Training accuracy: 0.8116022714619753\n",
      "Test loss: 0.655202746742256\n",
      "Test accuracy: 0.7301275760549558\n",
      "Epoch 4, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.45509832257245436\n",
      "Training accuracy: 0.821178042534239\n",
      "Test loss: 0.7983918034410337\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 5, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4679482022042981\n",
      "Training accuracy: 0.8179490034517314\n",
      "Test loss: 0.5338287748345215\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 6, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4608565131097604\n",
      "Training accuracy: 0.8202872731321679\n",
      "Test loss: 0.541253497778375\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 7, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4536229956323938\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.5896508359990948\n",
      "Test accuracy: 0.760549558390579\n",
      "Epoch 8, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.44156846871634603\n",
      "Training accuracy: 0.8258545818951119\n",
      "Test loss: 0.7376987129245118\n",
      "Test accuracy: 0.7497546614327772\n",
      "Epoch 9, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.45580036021489473\n",
      "Training accuracy: 0.8236276583899343\n",
      "Test loss: 0.579010593645472\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 10, lr: 0.005, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4394176634219983\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.5429251227926343\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 1, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5504726136318759\n",
      "Training accuracy: 0.7798686115131945\n",
      "Test loss: 0.7441519507599064\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 2, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4965941209979451\n",
      "Training accuracy: 0.8058122703485135\n",
      "Test loss: 0.8556272816260265\n",
      "Test accuracy: 0.6565260058881256\n",
      "Epoch 3, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.47333252339038695\n",
      "Training accuracy: 0.8126043870393052\n",
      "Test loss: 0.860085931622128\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 4, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4789324029359264\n",
      "Training accuracy: 0.8159447722970716\n",
      "Test loss: 0.6897589604683784\n",
      "Test accuracy: 0.760549558390579\n",
      "Epoch 5, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4707028402552521\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.6375235195716769\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 6, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4612675366157705\n",
      "Training accuracy: 0.8200645807816501\n",
      "Test loss: 0.5371555885459068\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 7, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4638626325082014\n",
      "Training accuracy: 0.822959581338381\n",
      "Test loss: 0.5099354834095653\n",
      "Test accuracy: 0.8007850834151129\n",
      "Epoch 8, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4674696454162548\n",
      "Training accuracy: 0.8216234272352745\n",
      "Test loss: 0.573232398007405\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 9, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.46242456691804806\n",
      "Training accuracy: 0.8258545818951119\n",
      "Test loss: 0.5604329362467765\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 10, lr: 0.006, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.461452677837659\n",
      "Training accuracy: 0.8209553501837212\n",
      "Test loss: 0.6351377645816373\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 1, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5614208131613783\n",
      "Training accuracy: 0.7769736109564637\n",
      "Test loss: 0.5980190146308652\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 2, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5095116362090154\n",
      "Training accuracy: 0.8010243848123817\n",
      "Test loss: 0.6749440206980681\n",
      "Test accuracy: 0.7330716388616291\n",
      "Epoch 3, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5057515734229052\n",
      "Training accuracy: 0.8074824629773967\n",
      "Test loss: 0.5820796067394148\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 4, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4899740856210953\n",
      "Training accuracy: 0.8107115020599043\n",
      "Test loss: 0.7192312309388674\n",
      "Test accuracy: 0.6997055937193327\n",
      "Epoch 5, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4798288410310689\n",
      "Training accuracy: 0.8139405411424118\n",
      "Test loss: 0.5257714484343936\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 6, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.48405404235761584\n",
      "Training accuracy: 0.8131611179155996\n",
      "Test loss: 0.6115906985515\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 7, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.47401233707002854\n",
      "Training accuracy: 0.8189511190290614\n",
      "Test loss: 0.5524783353517755\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 8, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.47492137639908283\n",
      "Training accuracy: 0.8171695802249193\n",
      "Test loss: 0.6761943877503729\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 9, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4708543018314624\n",
      "Training accuracy: 0.8226255428126044\n",
      "Test loss: 0.6239106598967308\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 10, lr: 0.007, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.46875732519249386\n",
      "Training accuracy: 0.8225141966373455\n",
      "Test loss: 0.5938310716758182\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 1, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5682166449782869\n",
      "Training accuracy: 0.7805366885647478\n",
      "Test loss: 0.6314873553819815\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 2, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5247238142866558\n",
      "Training accuracy: 0.8004676539360873\n",
      "Test loss: 0.6691054318674179\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 3, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5136024459156184\n",
      "Training accuracy: 0.8075938091526557\n",
      "Test loss: 0.631153104525202\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 4, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5234521148783794\n",
      "Training accuracy: 0.8029172697917827\n",
      "Test loss: 0.5650965354393462\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 5, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5113012928525213\n",
      "Training accuracy: 0.8055895779979958\n",
      "Test loss: 0.5740934441906196\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 6, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.4956610692116827\n",
      "Training accuracy: 0.8140518873176706\n",
      "Test loss: 0.6941795257286656\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 7, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5032771731415832\n",
      "Training accuracy: 0.8130497717403408\n",
      "Test loss: 0.7357746391932793\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 8, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5063677219568841\n",
      "Training accuracy: 0.8123816946887874\n",
      "Test loss: 0.5538105354575812\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 9, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.48546021595031486\n",
      "Training accuracy: 0.817392272575437\n",
      "Test loss: 0.5598542543458985\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 10, lr: 0.008, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.48567736479676177\n",
      "Training accuracy: 0.8147199643692239\n",
      "Test loss: 0.5760916604261052\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 1, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.6027785684042738\n",
      "Training accuracy: 0.7681772631110121\n",
      "Test loss: 0.7014486781221845\n",
      "Test accuracy: 0.7311089303238469\n",
      "Epoch 2, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5465186193424455\n",
      "Training accuracy: 0.7999109230597929\n",
      "Test loss: 0.5300167905453728\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 3, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5358372526184221\n",
      "Training accuracy: 0.8040307315443714\n",
      "Test loss: 0.7006791331115251\n",
      "Test accuracy: 0.7311089303238469\n",
      "Epoch 4, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5415052939013482\n",
      "Training accuracy: 0.8052555394722192\n",
      "Test loss: 0.5639557373418425\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.526512652340239\n",
      "Training accuracy: 0.8092640017815388\n",
      "Test loss: 0.6699888685149697\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 6, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5184480601729005\n",
      "Training accuracy: 0.8069257321011023\n",
      "Test loss: 0.9940806242972758\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 7, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5235902839932911\n",
      "Training accuracy: 0.8093753479567977\n",
      "Test loss: 0.6269301031007851\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 8, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5174377829403883\n",
      "Training accuracy: 0.8117136176372342\n",
      "Test loss: 0.6647244956823279\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 9, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.5070321767038949\n",
      "Training accuracy: 0.8131611179155996\n",
      "Test loss: 0.7453827548541779\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 10, lr: 0.009, gamma: 0.03\n",
      "-------------------------------\n",
      "Training loss: 0.49761944461036084\n",
      "Training accuracy: 0.8163901569981071\n",
      "Test loss: 0.573730596850267\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 1, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5500514383252858\n",
      "Training accuracy: 0.7800913038637123\n",
      "Test loss: 0.5300374806687689\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 2, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4790533286281981\n",
      "Training accuracy: 0.8093753479567977\n",
      "Test loss: 0.5240007295283298\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 3, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.46256401207077785\n",
      "Training accuracy: 0.8176149649259548\n",
      "Test loss: 0.5163001028256047\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 4, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.45235187815419947\n",
      "Training accuracy: 0.8202872731321679\n",
      "Test loss: 0.5137658092879688\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 5, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44816067594818726\n",
      "Training accuracy: 0.8226255428126044\n",
      "Test loss: 0.4993565236679822\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 6, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4432983109174785\n",
      "Training accuracy: 0.8238503507404521\n",
      "Test loss: 0.513053030214319\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 7, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.43935225242469905\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.49585593624133706\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 8, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4356544229819294\n",
      "Training accuracy: 0.8290836209776195\n",
      "Test loss: 0.51448499431086\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 9, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.433255366679887\n",
      "Training accuracy: 0.8316445830085737\n",
      "Test loss: 0.5101727209509999\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 10, lr: 0.001, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4311882020065028\n",
      "Training accuracy: 0.8314218906580559\n",
      "Test loss: 0.5183634360826285\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 1, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5256250073869674\n",
      "Training accuracy: 0.7864380358534684\n",
      "Test loss: 0.5366027365430415\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 2, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4665365367779394\n",
      "Training accuracy: 0.8152766952455183\n",
      "Test loss: 0.5283684294754905\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4525208423695417\n",
      "Training accuracy: 0.8203986193074267\n",
      "Test loss: 0.5473522208155781\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 4, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44716433565675806\n",
      "Training accuracy: 0.8237390045651932\n",
      "Test loss: 0.5137544012865211\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4442015779488284\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.5201238644134776\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 6, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4376832423152006\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.5014833869948121\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 7, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.43259259283562546\n",
      "Training accuracy: 0.8316445830085737\n",
      "Test loss: 0.5185122930968474\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 8, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4322641790775955\n",
      "Training accuracy: 0.8291949671528783\n",
      "Test loss: 0.5047678388837977\n",
      "Test accuracy: 0.803729146221786\n",
      "Epoch 9, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.42919373798643795\n",
      "Training accuracy: 0.8315332368333148\n",
      "Test loss: 0.519677783134992\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 10, lr: 0.002, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.42607713794272883\n",
      "Training accuracy: 0.8332034294621979\n",
      "Test loss: 0.5022150498318602\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 1, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5075249822722977\n",
      "Training accuracy: 0.7945663066473667\n",
      "Test loss: 0.5703790094246457\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 2, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.46666306236674004\n",
      "Training accuracy: 0.8143859258434473\n",
      "Test loss: 0.5451152209444766\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 3, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4535325235001641\n",
      "Training accuracy: 0.82206881193631\n",
      "Test loss: 0.5015421474061841\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 4, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44755386947497083\n",
      "Training accuracy: 0.822959581338381\n",
      "Test loss: 0.5254139570890395\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 5, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44135205825470747\n",
      "Training accuracy: 0.8266340051219241\n",
      "Test loss: 0.560082177009152\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 6, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4444793550690223\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.5157026437360709\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 7, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.43565525819193135\n",
      "Training accuracy: 0.8278588130497717\n",
      "Test loss: 0.5952894946192853\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 8, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.43348431607129484\n",
      "Training accuracy: 0.8324240062353858\n",
      "Test loss: 0.499823618566908\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 9, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4325090849098258\n",
      "Training accuracy: 0.8337601603384923\n",
      "Test loss: 0.5013613977539879\n",
      "Test accuracy: 0.7988223748773308\n",
      "Epoch 10, lr: 0.003, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.425382577948867\n",
      "Training accuracy: 0.8358757376684111\n",
      "Test loss: 0.534905354864347\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 1, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5235152591470753\n",
      "Training accuracy: 0.7915599599153769\n",
      "Test loss: 0.5253966283400463\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 2, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4677214682161934\n",
      "Training accuracy: 0.8152766952455183\n",
      "Test loss: 0.6958654491601912\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 3, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.460808166624934\n",
      "Training accuracy: 0.8154993875960361\n",
      "Test loss: 0.6107621680766022\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 4, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4504509447475356\n",
      "Training accuracy: 0.8249638124930408\n",
      "Test loss: 0.5912362089100951\n",
      "Test accuracy: 0.7625122669283612\n",
      "Epoch 5, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44769274343919707\n",
      "Training accuracy: 0.8252978510188175\n",
      "Test loss: 0.5188200399791411\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 6, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4449324139029156\n",
      "Training accuracy: 0.8270793898229596\n",
      "Test loss: 0.5312061278228086\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44686818828018376\n",
      "Training accuracy: 0.8249638124930408\n",
      "Test loss: 0.5184959337568611\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 8, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44186627789178706\n",
      "Training accuracy: 0.8256318895445941\n",
      "Test loss: 0.5124357918878767\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 9, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4394722085675502\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.5426021385883092\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 10, lr: 0.004, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.442061091754235\n",
      "Training accuracy: 0.8256318895445941\n",
      "Test loss: 0.5842258570765139\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 1, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5382408277851186\n",
      "Training accuracy: 0.7843224585235498\n",
      "Test loss: 0.5944816235003223\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 2, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.48432471088226353\n",
      "Training accuracy: 0.8085959247299855\n",
      "Test loss: 0.5244759873974194\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 3, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4726168189623775\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.6392278075803134\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 4, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4691748127860596\n",
      "Training accuracy: 0.8189511190290614\n",
      "Test loss: 0.532576736044486\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 5, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4572151138666756\n",
      "Training accuracy: 0.8238503507404521\n",
      "Test loss: 0.7572571338685384\n",
      "Test accuracy: 0.6820412168792934\n",
      "Epoch 6, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.46161298612592055\n",
      "Training accuracy: 0.8216234272352745\n",
      "Test loss: 0.5235386796741655\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 7, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.456263484136921\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5622363246107241\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 8, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.44827615096459955\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.5095769648346045\n",
      "Test accuracy: 0.7998037291462218\n",
      "Epoch 9, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4481863395273918\n",
      "Training accuracy: 0.8256318895445941\n",
      "Test loss: 0.5444867604497137\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 10, lr: 0.005, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4370422645608562\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.6937380337738547\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 1, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5424968997217103\n",
      "Training accuracy: 0.786883420554504\n",
      "Test loss: 0.5542249704717537\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 2, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.49305695770525104\n",
      "Training accuracy: 0.8064803474000668\n",
      "Test loss: 0.555486103503346\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 3, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.47932328517566264\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.5516665501847234\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 4, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.48076794091344555\n",
      "Training accuracy: 0.8181716958022492\n",
      "Test loss: 0.6692896020307157\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 5, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.47798626920107334\n",
      "Training accuracy: 0.8158334261218128\n",
      "Test loss: 0.578114168274274\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 6, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4599679863515297\n",
      "Training accuracy: 0.8193965037300969\n",
      "Test loss: 0.5425401722484061\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 7, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.46917293020245654\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.6917867739488379\n",
      "Test accuracy: 0.719332679097154\n",
      "Epoch 8, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4577911570726292\n",
      "Training accuracy: 0.8251865048435586\n",
      "Test loss: 0.5456185112172652\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 9, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.45841391880200844\n",
      "Training accuracy: 0.8234049660394166\n",
      "Test loss: 0.5217975057200899\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 10, lr: 0.006, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4678391591104364\n",
      "Training accuracy: 0.8218461195857922\n",
      "Test loss: 0.5435130528308693\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 1, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5843159167939923\n",
      "Training accuracy: 0.7754147645028393\n",
      "Test loss: 1.4481008855353628\n",
      "Test accuracy: 0.7173699705593719\n",
      "Epoch 2, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5188602653819752\n",
      "Training accuracy: 0.8032513083175593\n",
      "Test loss: 0.5288228291407652\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 3, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5029539484288031\n",
      "Training accuracy: 0.8032513083175593\n",
      "Test loss: 0.6509494304189036\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 4, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.48866996716290245\n",
      "Training accuracy: 0.8148313105444828\n",
      "Test loss: 0.8662517215248647\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 5, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4881999394686845\n",
      "Training accuracy: 0.8163901569981071\n",
      "Test loss: 0.5579276038105284\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 6, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4850772799700648\n",
      "Training accuracy: 0.814608618193965\n",
      "Test loss: 0.9701790566299333\n",
      "Test accuracy: 0.732090284592738\n",
      "Epoch 7, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4812591093617337\n",
      "Training accuracy: 0.8170582340496604\n",
      "Test loss: 0.6754711594385067\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 8, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.48383802419754224\n",
      "Training accuracy: 0.8161674646475894\n",
      "Test loss: 0.563676114990152\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 9, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.47679798629073916\n",
      "Training accuracy: 0.8197305422558735\n",
      "Test loss: 0.6033365956934441\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 10, lr: 0.007, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.47048099423792805\n",
      "Training accuracy: 0.8230709275136399\n",
      "Test loss: 0.5675109261326982\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 1, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5869530651288234\n",
      "Training accuracy: 0.7768622647812048\n",
      "Test loss: 0.5766108348980268\n",
      "Test accuracy: 0.7625122669283612\n",
      "Epoch 2, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5513718580531989\n",
      "Training accuracy: 0.7997995768845341\n",
      "Test loss: 0.7605587373888879\n",
      "Test accuracy: 0.7409224730127576\n",
      "Epoch 3, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5169686088107245\n",
      "Training accuracy: 0.8080391938536912\n",
      "Test loss: 0.691938973912079\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 4, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5235729921091082\n",
      "Training accuracy: 0.8067030397505845\n",
      "Test loss: 0.6046224182674531\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 5, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5011953144200381\n",
      "Training accuracy: 0.8100434250083509\n",
      "Test loss: 0.6837896146647946\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 6, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4969007857670671\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.8029385494530727\n",
      "Test accuracy: 0.7477919528949951\n",
      "Epoch 7, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.51222904931595\n",
      "Training accuracy: 0.8094866941320565\n",
      "Test loss: 0.7554149573286804\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 8, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.49571838086623815\n",
      "Training accuracy: 0.8178376572764725\n",
      "Test loss: 1.4948469557167854\n",
      "Test accuracy: 0.549558390578999\n",
      "Epoch 9, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.499638066301366\n",
      "Training accuracy: 0.8139405411424118\n",
      "Test loss: 0.5776693661851667\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 10, lr: 0.008, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5063255848179494\n",
      "Training accuracy: 0.8154993875960361\n",
      "Test loss: 0.6294323369144107\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 1, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.6162679244781467\n",
      "Training accuracy: 0.7714063021935197\n",
      "Test loss: 0.6801985823371108\n",
      "Test accuracy: 0.7203140333660452\n",
      "Epoch 2, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5665671408090145\n",
      "Training accuracy: 0.7932301525442601\n",
      "Test loss: 0.7218558945061531\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 3, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5489724730180348\n",
      "Training accuracy: 0.7981293842556508\n",
      "Test loss: 0.545541458380234\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 4, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5346722515040047\n",
      "Training accuracy: 0.8051441932969603\n",
      "Test loss: 0.5350057345260231\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 5, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5301072154102446\n",
      "Training accuracy: 0.8108228482351632\n",
      "Test loss: 0.6831251483436188\n",
      "Test accuracy: 0.746810598626104\n",
      "Epoch 6, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5382327802422026\n",
      "Training accuracy: 0.8046988085959247\n",
      "Test loss: 0.6660230406016199\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 7, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5272635243714433\n",
      "Training accuracy: 0.8073711168021378\n",
      "Test loss: 0.6176791075284395\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 8, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5098618278390858\n",
      "Training accuracy: 0.8122703485135285\n",
      "Test loss: 0.5632122294123671\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 9, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.5157788672766831\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.6464996850291693\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 10, lr: 0.009, gamma: 0.04\n",
      "-------------------------------\n",
      "Training loss: 0.4961173844395867\n",
      "Training accuracy: 0.8187284266785436\n",
      "Test loss: 0.6327560790855588\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 1, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5505386290777474\n",
      "Training accuracy: 0.7788664959358647\n",
      "Test loss: 0.5358685938252084\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 2, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4807558672264336\n",
      "Training accuracy: 0.8103774635341277\n",
      "Test loss: 0.5141792092929294\n",
      "Test accuracy: 0.7998037291462218\n",
      "Epoch 3, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.46498956788211393\n",
      "Training accuracy: 0.8182830419775081\n",
      "Test loss: 0.53526169476963\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 4, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4555943181375103\n",
      "Training accuracy: 0.8195078499053557\n",
      "Test loss: 0.5070727533634324\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 5, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.45005381949865636\n",
      "Training accuracy: 0.8227368889878632\n",
      "Test loss: 0.5020124770708243\n",
      "Test accuracy: 0.7998037291462218\n",
      "Epoch 6, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.44495907126563045\n",
      "Training accuracy: 0.8231822736888987\n",
      "Test loss: 0.5252918167015989\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 7, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4421028844890238\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.499572141715659\n",
      "Test accuracy: 0.7988223748773308\n",
      "Epoch 8, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4380946405453454\n",
      "Training accuracy: 0.8297516980291727\n",
      "Test loss: 0.5182000370559093\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 9, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4355121335872708\n",
      "Training accuracy: 0.8291949671528783\n",
      "Test loss: 0.5084695449579686\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 10, lr: 0.001, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43317348236343983\n",
      "Training accuracy: 0.8299743903796905\n",
      "Test loss: 0.5031209265752911\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 1, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.523676786772088\n",
      "Training accuracy: 0.7872174590802806\n",
      "Test loss: 0.5297364491358824\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 2, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.47167474474451376\n",
      "Training accuracy: 0.8139405411424118\n",
      "Test loss: 0.5100836927862701\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 3, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.45658545304733134\n",
      "Training accuracy: 0.8219574657610511\n",
      "Test loss: 0.5165959547499562\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 4, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4508142525369799\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.5161438112579454\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.44473467050303783\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.5122607621173279\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 6, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4380321871589252\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.5197721854861281\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43735928454692186\n",
      "Training accuracy: 0.829529005678655\n",
      "Test loss: 0.5808850998265469\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 8, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4348482248472777\n",
      "Training accuracy: 0.8291949671528783\n",
      "Test loss: 0.5050759672884618\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 9, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43244601411326217\n",
      "Training accuracy: 0.8307538136065026\n",
      "Test loss: 0.5282232991261618\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 10, lr: 0.002, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43024062993544776\n",
      "Training accuracy: 0.8319786215343503\n",
      "Test loss: 0.5554532683632676\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 1, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5259498535239502\n",
      "Training accuracy: 0.7893330364101994\n",
      "Test loss: 0.5881697812188011\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 2, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.46695695845510815\n",
      "Training accuracy: 0.8147199643692239\n",
      "Test loss: 0.5202618212063483\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.45727568741047014\n",
      "Training accuracy: 0.8215120810600156\n",
      "Test loss: 0.5459072091511585\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 4, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.44887684309698933\n",
      "Training accuracy: 0.8254091971940763\n",
      "Test loss: 0.5357784578856355\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 5, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4456230936151559\n",
      "Training accuracy: 0.8270793898229596\n",
      "Test loss: 0.5360761267280204\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 6, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.44380032102988143\n",
      "Training accuracy: 0.8236276583899343\n",
      "Test loss: 0.5592768811150084\n",
      "Test accuracy: 0.7625122669283612\n",
      "Epoch 7, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4372621991894264\n",
      "Training accuracy: 0.8300857365549493\n",
      "Test loss: 0.569745222941933\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 8, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43622683958438624\n",
      "Training accuracy: 0.8308651597817615\n",
      "Test loss: 0.4956106174448405\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 9, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4298370822628653\n",
      "Training accuracy: 0.8324240062353858\n",
      "Test loss: 0.500409106920465\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 10, lr: 0.003, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.43358675108503064\n",
      "Training accuracy: 0.8307538136065026\n",
      "Test loss: 0.5674138756156318\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 1, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.526128699301399\n",
      "Training accuracy: 0.7875514976060572\n",
      "Test loss: 0.5448926632841857\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 2, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4827847956894315\n",
      "Training accuracy: 0.8097093864825743\n",
      "Test loss: 0.5078796551318819\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4657155362087161\n",
      "Training accuracy: 0.8191738113795791\n",
      "Test loss: 0.6096874206410072\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 4, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4572922230802179\n",
      "Training accuracy: 0.8240730430909698\n",
      "Test loss: 0.642612392113885\n",
      "Test accuracy: 0.7232580961727183\n",
      "Epoch 5, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4519677950997831\n",
      "Training accuracy: 0.8231822736888987\n",
      "Test loss: 0.5054406516685336\n",
      "Test accuracy: 0.7998037291462218\n",
      "Epoch 6, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4489047743007582\n",
      "Training accuracy: 0.8250751586682997\n",
      "Test loss: 0.5149686007200209\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 7, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4473205088586853\n",
      "Training accuracy: 0.8250751586682997\n",
      "Test loss: 0.543917604178278\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 8, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4398385415150479\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.5261170402008372\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 9, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4395925650888901\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.5631280127287613\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 10, lr: 0.004, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4419041428026013\n",
      "Training accuracy: 0.8249638124930408\n",
      "Test loss: 0.5691797250152453\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 1, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5416107886273722\n",
      "Training accuracy: 0.7842111123482909\n",
      "Test loss: 0.5355136984697852\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 2, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.48805331004321634\n",
      "Training accuracy: 0.8087072709052444\n",
      "Test loss: 0.6020057686294727\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 3, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4791645147648862\n",
      "Training accuracy: 0.812715733214564\n",
      "Test loss: 0.6391404282824448\n",
      "Test accuracy: 0.7536800785083415\n",
      "Epoch 4, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4635337226050506\n",
      "Training accuracy: 0.8185057343280259\n",
      "Test loss: 0.5664702164881129\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 5, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4612376749608448\n",
      "Training accuracy: 0.8219574657610511\n",
      "Test loss: 0.7312729196646731\n",
      "Test accuracy: 0.7409224730127576\n",
      "Epoch 6, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4619860820834773\n",
      "Training accuracy: 0.8199532346063912\n",
      "Test loss: 0.5302222103787123\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 7, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.45567256626367647\n",
      "Training accuracy: 0.8259659280703708\n",
      "Test loss: 0.5310890612003263\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 8, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.45251794922472627\n",
      "Training accuracy: 0.8266340051219241\n",
      "Test loss: 0.5812993636894039\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 9, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4501532891547676\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.7880006361171472\n",
      "Test accuracy: 0.7409224730127576\n",
      "Epoch 10, lr: 0.005, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.44699343881340525\n",
      "Training accuracy: 0.8297516980291727\n",
      "Test loss: 0.5727599730844938\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 1, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5413268185905592\n",
      "Training accuracy: 0.7902238058122704\n",
      "Test loss: 0.5754403949836754\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 2, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5000350944927654\n",
      "Training accuracy: 0.8078165015031734\n",
      "Test loss: 0.6418386104647381\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 3, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.49348565071651973\n",
      "Training accuracy: 0.8108228482351632\n",
      "Test loss: 0.7403233777786494\n",
      "Test accuracy: 0.6987242394504416\n",
      "Epoch 4, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.48808250113593327\n",
      "Training accuracy: 0.8148313105444828\n",
      "Test loss: 0.6083595143801097\n",
      "Test accuracy: 0.746810598626104\n",
      "Epoch 5, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4770248376530723\n",
      "Training accuracy: 0.8167241955238838\n",
      "Test loss: 0.5768425578692008\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 6, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.47683346426482986\n",
      "Training accuracy: 0.8176149649259548\n",
      "Test loss: 0.7010156901825633\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 7, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4714752444710183\n",
      "Training accuracy: 0.8202872731321679\n",
      "Test loss: 0.5418052469198322\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 8, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.46771390772107424\n",
      "Training accuracy: 0.8209553501837212\n",
      "Test loss: 0.6017530157474821\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 9, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4593282358548965\n",
      "Training accuracy: 0.8250751586682997\n",
      "Test loss: 0.6416414312806283\n",
      "Test accuracy: 0.7252208047105005\n",
      "Epoch 10, lr: 0.006, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.47790406649155975\n",
      "Training accuracy: 0.818394388152767\n",
      "Test loss: 0.6151829567811908\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 1, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.576044752243141\n",
      "Training accuracy: 0.7819841888431133\n",
      "Test loss: 0.6504896580295544\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 2, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.513690760868746\n",
      "Training accuracy: 0.8023605389154883\n",
      "Test loss: 0.5736508030769752\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.49564283937232395\n",
      "Training accuracy: 0.8110455405856809\n",
      "Test loss: 0.8636616655608038\n",
      "Test accuracy: 0.7242394504416094\n",
      "Epoch 4, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4960918137284141\n",
      "Training accuracy: 0.8064803474000668\n",
      "Test loss: 0.5688577875187867\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 5, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5014983374455793\n",
      "Training accuracy: 0.8085959247299855\n",
      "Test loss: 0.6278300015392903\n",
      "Test accuracy: 0.7389597644749755\n",
      "Epoch 6, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4891987002986986\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.6364441951544877\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 7, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4967109526156742\n",
      "Training accuracy: 0.8130497717403408\n",
      "Test loss: 0.5729423055353997\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 8, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.49809329802548485\n",
      "Training accuracy: 0.8111568867609398\n",
      "Test loss: 1.0449796462316392\n",
      "Test accuracy: 0.6104023552502453\n",
      "Epoch 9, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5095529984145637\n",
      "Training accuracy: 0.8089299632557622\n",
      "Test loss: 0.5896630962182776\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 10, lr: 0.007, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.47991232576474835\n",
      "Training accuracy: 0.8197305422558735\n",
      "Test loss: 0.631465443802067\n",
      "Test accuracy: 0.7477919528949951\n",
      "Epoch 1, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5926960738268559\n",
      "Training accuracy: 0.774746687451286\n",
      "Test loss: 0.6565439089240691\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 2, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5364563217965868\n",
      "Training accuracy: 0.8009130386371228\n",
      "Test loss: 0.5821459107941804\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5398283109555012\n",
      "Training accuracy: 0.8019151542144527\n",
      "Test loss: 0.5581306079448264\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 4, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5127458202256191\n",
      "Training accuracy: 0.8100434250083509\n",
      "Test loss: 1.029105389785018\n",
      "Test accuracy: 0.7212953876349362\n",
      "Epoch 5, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4991459049223898\n",
      "Training accuracy: 0.8151653490702594\n",
      "Test loss: 0.6366048358724442\n",
      "Test accuracy: 0.7379784102060843\n",
      "Epoch 6, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4940408839539395\n",
      "Training accuracy: 0.8200645807816501\n",
      "Test loss: 0.5635232285206171\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 7, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.49271993795255287\n",
      "Training accuracy: 0.8140518873176706\n",
      "Test loss: 0.5639133372648425\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 8, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5118167297158657\n",
      "Training accuracy: 0.8070370782763612\n",
      "Test loss: 0.5780724342016758\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 9, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.4954938183204393\n",
      "Training accuracy: 0.8138291949671529\n",
      "Test loss: 0.6061220619465582\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 10, lr: 0.008, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.49152428338667314\n",
      "Training accuracy: 0.815610733771295\n",
      "Test loss: 0.552300686077\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 1, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.6112404609294607\n",
      "Training accuracy: 0.7759714953791337\n",
      "Test loss: 0.6561229915976875\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 2, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5527888464975458\n",
      "Training accuracy: 0.79523438369892\n",
      "Test loss: 0.8912102205128618\n",
      "Test accuracy: 0.7340529931305201\n",
      "Epoch 3, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5587603563641721\n",
      "Training accuracy: 0.7962364992762498\n",
      "Test loss: 0.9981617023245275\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 4, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5257782621846115\n",
      "Training accuracy: 0.8072597706268789\n",
      "Test loss: 0.5684729369395615\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 5, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5405443789778169\n",
      "Training accuracy: 0.8046988085959247\n",
      "Test loss: 1.5503558379628122\n",
      "Test accuracy: 0.7134445534838076\n",
      "Epoch 6, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5443634136643498\n",
      "Training accuracy: 0.8065916935753257\n",
      "Test loss: 0.6806166848794751\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 7, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5070343949510451\n",
      "Training accuracy: 0.8122703485135285\n",
      "Test loss: 0.6413957420462364\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 8, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.50820319175176\n",
      "Training accuracy: 0.8140518873176706\n",
      "Test loss: 0.5938178438901667\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 9, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.5287924012035055\n",
      "Training accuracy: 0.8110455405856809\n",
      "Test loss: 0.6182587530474901\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 10, lr: 0.009, gamma: 0.05\n",
      "-------------------------------\n",
      "Training loss: 0.502902697928166\n",
      "Training accuracy: 0.8121590023382697\n",
      "Test loss: 0.5802373471976029\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 1, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5469218975862381\n",
      "Training accuracy: 0.7795345729874179\n",
      "Test loss: 0.5500985689614776\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 2, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4863690161722273\n",
      "Training accuracy: 0.8057009241732547\n",
      "Test loss: 0.5333507039947061\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 3, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.469584323929567\n",
      "Training accuracy: 0.8168355416991426\n",
      "Test loss: 0.5220228113065875\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4601517815046222\n",
      "Training accuracy: 0.8185057343280259\n",
      "Test loss: 0.5115049684656029\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4543305147883489\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5213122912431255\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 6, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44936031940691906\n",
      "Training accuracy: 0.824852466317782\n",
      "Test loss: 0.5226624671797523\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 7, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4441047108972435\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.5315747571061709\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 8, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44257770489012394\n",
      "Training accuracy: 0.8283041977508072\n",
      "Test loss: 0.507047773518904\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 9, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43993133629215225\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.5032413226348496\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 10, lr: 0.001, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43650442737471673\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.504027413491295\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 1, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5335459781010291\n",
      "Training accuracy: 0.7852132279256208\n",
      "Test loss: 0.5404828661381906\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 2, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.47402421427442054\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.5486728834450771\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 3, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.46298582402081073\n",
      "Training accuracy: 0.8221801581115689\n",
      "Test loss: 0.5218057446437682\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4540092054248029\n",
      "Training accuracy: 0.824852466317782\n",
      "Test loss: 0.5145442329047356\n",
      "Test accuracy: 0.7988223748773308\n",
      "Epoch 5, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44698068158104376\n",
      "Training accuracy: 0.8266340051219241\n",
      "Test loss: 0.5154067314174154\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 6, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44348796406695895\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.504667863586115\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4427323433885913\n",
      "Training accuracy: 0.8273020821734773\n",
      "Test loss: 0.5332656439258493\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 8, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43558994325659955\n",
      "Training accuracy: 0.830419775080726\n",
      "Test loss: 0.5170624414358336\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 9, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4314935856507385\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.5151259631825148\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 10, lr: 0.002, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43098547461701586\n",
      "Training accuracy: 0.8330920832869391\n",
      "Test loss: 0.54871423099413\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 1, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5278971614298477\n",
      "Training accuracy: 0.7888876517091638\n",
      "Test loss: 0.5258960429363325\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 2, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4722675084935014\n",
      "Training accuracy: 0.8130497717403408\n",
      "Test loss: 0.5511651246774653\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.46054812775676546\n",
      "Training accuracy: 0.8181716958022492\n",
      "Test loss: 0.574714970658876\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 4, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4524994494704358\n",
      "Training accuracy: 0.8216234272352745\n",
      "Test loss: 0.5048195605072119\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 5, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44944555437746697\n",
      "Training accuracy: 0.8246297739672642\n",
      "Test loss: 0.5183233886050523\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 6, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44714155484913\n",
      "Training accuracy: 0.8256318895445941\n",
      "Test loss: 0.5440758044399153\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 7, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4395741158103959\n",
      "Training accuracy: 0.8293063133281372\n",
      "Test loss: 0.5499214113519049\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 8, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43786713698302937\n",
      "Training accuracy: 0.830419775080726\n",
      "Test loss: 0.5177743439293001\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 9, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43627064134280596\n",
      "Training accuracy: 0.8342055450395279\n",
      "Test loss: 0.5181092877266334\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 10, lr: 0.003, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.43459839049538823\n",
      "Training accuracy: 0.830419775080726\n",
      "Test loss: 0.5422946786155176\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 1, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.526201001298466\n",
      "Training accuracy: 0.7897784211112349\n",
      "Test loss: 0.5355241654899567\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 2, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4780072764204308\n",
      "Training accuracy: 0.8112682329361987\n",
      "Test loss: 0.5101094334820419\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 3, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4675386662327119\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.524630320142366\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 4, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4563359404948198\n",
      "Training accuracy: 0.825743235719853\n",
      "Test loss: 0.5816733707857553\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 5, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4529124264886472\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.5393789863329055\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 6, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.45204949027385494\n",
      "Training accuracy: 0.8242957354414876\n",
      "Test loss: 0.5105742269339107\n",
      "Test accuracy: 0.7978410206084396\n",
      "Epoch 7, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44639574108935054\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.528372300706504\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 8, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44469071062738624\n",
      "Training accuracy: 0.8271907359982185\n",
      "Test loss: 0.5374549330743185\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 9, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44210257240584666\n",
      "Training accuracy: 0.8305311212559848\n",
      "Test loss: 0.5437391068680364\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 10, lr: 0.004, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.44821429829358017\n",
      "Training accuracy: 0.8255205433693352\n",
      "Test loss: 0.5294341671923075\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 1, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5442967200090753\n",
      "Training accuracy: 0.7852132279256208\n",
      "Test loss: 0.7130904666223985\n",
      "Test accuracy: 0.6928361138370952\n",
      "Epoch 2, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.48715434371993693\n",
      "Training accuracy: 0.8102661173588688\n",
      "Test loss: 0.6658117244124295\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 3, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4820696369979923\n",
      "Training accuracy: 0.8172809264001781\n",
      "Test loss: 0.5650772589521156\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 4, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4631625341354984\n",
      "Training accuracy: 0.817392272575437\n",
      "Test loss: 0.5177186039642918\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 5, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.46502301286727227\n",
      "Training accuracy: 0.8206213116579445\n",
      "Test loss: 0.5606894515330471\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 6, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4608725143021829\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.5381446944017757\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 7, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4631487933103551\n",
      "Training accuracy: 0.8215120810600156\n",
      "Test loss: 0.5210826077568871\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 8, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.45707399512074626\n",
      "Training accuracy: 0.8251865048435586\n",
      "Test loss: 0.525175276752543\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 9, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.45836301324309386\n",
      "Training accuracy: 0.8208440040084624\n",
      "Test loss: 0.6011197585703931\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 10, lr: 0.005, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.460202897783643\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.5222143183980535\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 1, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5550184780552397\n",
      "Training accuracy: 0.7861039973276918\n",
      "Test loss: 0.5928558061471514\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 2, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5072822638097418\n",
      "Training accuracy: 0.8060349626990313\n",
      "Test loss: 0.5302837171662194\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 3, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4860990309274591\n",
      "Training accuracy: 0.8128270793898229\n",
      "Test loss: 0.5796882028200675\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 4, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.476589492787339\n",
      "Training accuracy: 0.8172809264001781\n",
      "Test loss: 0.5979198449025327\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 5, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4760730186664584\n",
      "Training accuracy: 0.817392272575437\n",
      "Test loss: 0.563218840877722\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 6, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4682017519112725\n",
      "Training accuracy: 0.82206881193631\n",
      "Test loss: 0.6044169040552182\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 7, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.47255622845505196\n",
      "Training accuracy: 0.8191738113795791\n",
      "Test loss: 0.8219365608001948\n",
      "Test accuracy: 0.7389597644749755\n",
      "Epoch 8, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.46430875100971075\n",
      "Training accuracy: 0.8216234272352745\n",
      "Test loss: 0.5367359239406979\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 9, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4690447983876088\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.8998838699367026\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 10, lr: 0.006, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4745383012893404\n",
      "Training accuracy: 0.8196191960806146\n",
      "Test loss: 0.5639315756427177\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 1, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5604494865089837\n",
      "Training accuracy: 0.783097650595702\n",
      "Test loss: 0.5380814475563955\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 2, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5291939953323621\n",
      "Training accuracy: 0.8029172697917827\n",
      "Test loss: 0.5535767654558393\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 3, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5169085784294786\n",
      "Training accuracy: 0.806257655049549\n",
      "Test loss: 0.6927057390007116\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 4, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5030036882692636\n",
      "Training accuracy: 0.8108228482351632\n",
      "Test loss: 0.5921929163366585\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 5, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4808035857785506\n",
      "Training accuracy: 0.8187284266785436\n",
      "Test loss: 0.6500155725118807\n",
      "Test accuracy: 0.7369970559371933\n",
      "Epoch 6, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4849661273356868\n",
      "Training accuracy: 0.8203986193074267\n",
      "Test loss: 0.5963891695479299\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 7, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4944439978253641\n",
      "Training accuracy: 0.811824963812493\n",
      "Test loss: 0.645143983174587\n",
      "Test accuracy: 0.7428851815505397\n",
      "Epoch 8, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.48991981462638023\n",
      "Training accuracy: 0.810934194410422\n",
      "Test loss: 0.6184965460117946\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 9, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.4908020214638404\n",
      "Training accuracy: 0.813717848791894\n",
      "Test loss: 0.8532061974597516\n",
      "Test accuracy: 0.7242394504416094\n",
      "Epoch 10, lr: 0.007, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.47879986078220704\n",
      "Training accuracy: 0.8202872731321679\n",
      "Test loss: 0.8918074968986586\n",
      "Test accuracy: 0.7271835132482827\n",
      "Epoch 1, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.6031714056113185\n",
      "Training accuracy: 0.7740786103997328\n",
      "Test loss: 0.5857294438163713\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 2, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5175678502038651\n",
      "Training accuracy: 0.8022491927402293\n",
      "Test loss: 0.7519650946538512\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 3, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.535848280997521\n",
      "Training accuracy: 0.8015811156886761\n",
      "Test loss: 0.7496950295886301\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 4, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5210880653130266\n",
      "Training accuracy: 0.8092640017815388\n",
      "Test loss: 0.6761729371559397\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 5, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5084172338844607\n",
      "Training accuracy: 0.8095980403073154\n",
      "Test loss: 0.6015734706472485\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 6, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5164186847845731\n",
      "Training accuracy: 0.8089299632557622\n",
      "Test loss: 0.5215561921334477\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 7, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5249646843415487\n",
      "Training accuracy: 0.8088186170805033\n",
      "Test loss: 0.7377549250764631\n",
      "Test accuracy: 0.7095191364082434\n",
      "Epoch 8, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5104233500700248\n",
      "Training accuracy: 0.8107115020599043\n",
      "Test loss: 0.613151582113309\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 9, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.48325956904427564\n",
      "Training accuracy: 0.8195078499053557\n",
      "Test loss: 0.5593866845108459\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 10, lr: 0.008, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5087558649394894\n",
      "Training accuracy: 0.8132724640908585\n",
      "Test loss: 0.6401321951142706\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 1, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.6232471354102493\n",
      "Training accuracy: 0.7712949560182608\n",
      "Test loss: 0.9627450575304453\n",
      "Test accuracy: 0.6486751717369971\n",
      "Epoch 2, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5525533422013059\n",
      "Training accuracy: 0.8012470771628994\n",
      "Test loss: 1.1010519491912572\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 3, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5373037844123135\n",
      "Training accuracy: 0.8014697695134172\n",
      "Test loss: 1.216711188204974\n",
      "Test accuracy: 0.704612365063788\n",
      "Epoch 4, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5362704067463308\n",
      "Training accuracy: 0.8029172697917827\n",
      "Test loss: 0.5732004106688195\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 5, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5326099602619426\n",
      "Training accuracy: 0.8055895779979958\n",
      "Test loss: 0.6260087547990127\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 6, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.544257142805347\n",
      "Training accuracy: 0.8038080391938537\n",
      "Test loss: 0.5862111143906755\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 7, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5165817506421371\n",
      "Training accuracy: 0.8120476561630108\n",
      "Test loss: 0.744624624132993\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 8, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5270459837842367\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.7292403019270087\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 9, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.5174201734811184\n",
      "Training accuracy: 0.8120476561630108\n",
      "Test loss: 1.0749580662430678\n",
      "Test accuracy: 0.7409224730127576\n",
      "Epoch 10, lr: 0.009, gamma: 0.06\n",
      "-------------------------------\n",
      "Training loss: 0.552735904319397\n",
      "Training accuracy: 0.8032513083175593\n",
      "Test loss: 0.6225482380425263\n",
      "Test accuracy: 0.7517173699705594\n",
      "Epoch 1, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5634304463080236\n",
      "Training accuracy: 0.7716289945440374\n",
      "Test loss: 0.549692317653334\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 2, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4911186059231934\n",
      "Training accuracy: 0.8045874624206658\n",
      "Test loss: 0.5297572100361383\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 3, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4747038426211809\n",
      "Training accuracy: 0.8144972720187061\n",
      "Test loss: 0.5211442221720155\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4646597290514254\n",
      "Training accuracy: 0.8193965037300969\n",
      "Test loss: 0.5249794115028157\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4586305616261924\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5153036897853968\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 6, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.45244720360176455\n",
      "Training accuracy: 0.8271907359982185\n",
      "Test loss: 0.5243149164321963\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 7, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44931310784934875\n",
      "Training accuracy: 0.8265226589466652\n",
      "Test loss: 0.5136135806337774\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 8, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44481778586843923\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5048441474524283\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 9, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4413411385391035\n",
      "Training accuracy: 0.8275247745239951\n",
      "Test loss: 0.5013182273205878\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 10, lr: 0.001, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.43862421598748763\n",
      "Training accuracy: 0.827636120699254\n",
      "Test loss: 0.501405576084968\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 1, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5372139724939414\n",
      "Training accuracy: 0.7875514976060572\n",
      "Test loss: 0.5525899578006509\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 2, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4792356258669909\n",
      "Training accuracy: 0.8108228482351632\n",
      "Test loss: 0.5239240379457502\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 3, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4635037562620426\n",
      "Training accuracy: 0.8175036187506959\n",
      "Test loss: 0.5224723746310974\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 4, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4563313442705575\n",
      "Training accuracy: 0.8236276583899343\n",
      "Test loss: 0.5120028057615469\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 5, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4486685287327241\n",
      "Training accuracy: 0.8260772742456297\n",
      "Test loss: 0.5119181072396081\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 6, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.442819600394117\n",
      "Training accuracy: 0.8293063133281372\n",
      "Test loss: 0.5342966446815692\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 7, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4391202773227284\n",
      "Training accuracy: 0.8315332368333148\n",
      "Test loss: 0.5269755903006302\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 8, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4414388483582885\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.5295414407541053\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 9, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4385644422682074\n",
      "Training accuracy: 0.8313105444827971\n",
      "Test loss: 0.5060956318432263\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 10, lr: 0.002, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4342473786029342\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.5265570330315646\n",
      "Test accuracy: 0.7998037291462218\n",
      "Epoch 1, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5314616406399268\n",
      "Training accuracy: 0.789667074935976\n",
      "Test loss: 0.5512301708636503\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 2, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.47624362767399747\n",
      "Training accuracy: 0.8141632334929295\n",
      "Test loss: 0.518965358472081\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 3, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4652573022070155\n",
      "Training accuracy: 0.821178042534239\n",
      "Test loss: 0.5118364823051711\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 4, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4556243739206692\n",
      "Training accuracy: 0.8221801581115689\n",
      "Test loss: 0.5232744886666917\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.452090179043482\n",
      "Training accuracy: 0.8260772742456297\n",
      "Test loss: 0.5279667409058291\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 6, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4458292094965101\n",
      "Training accuracy: 0.8296403518539138\n",
      "Test loss: 0.5309736877241126\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 7, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44245294621468334\n",
      "Training accuracy: 0.8281928515755483\n",
      "Test loss: 0.7675476307021974\n",
      "Test accuracy: 0.6879293424926398\n",
      "Epoch 8, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44319310537784673\n",
      "Training accuracy: 0.824852466317782\n",
      "Test loss: 0.5735679249534195\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 9, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4384019076956923\n",
      "Training accuracy: 0.8313105444827971\n",
      "Test loss: 0.5280351100070905\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 10, lr: 0.003, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.43564621203063314\n",
      "Training accuracy: 0.8298630442044316\n",
      "Test loss: 0.5598206156252411\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 1, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.532304750928607\n",
      "Training accuracy: 0.7908918828638236\n",
      "Test loss: 0.5304946840335857\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 2, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4782270799632487\n",
      "Training accuracy: 0.8128270793898229\n",
      "Test loss: 0.6590101981537382\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 3, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.47138588997399455\n",
      "Training accuracy: 0.8143859258434473\n",
      "Test loss: 0.6130973997949033\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 4, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4619302155180161\n",
      "Training accuracy: 0.8215120810600156\n",
      "Test loss: 0.6160070696686623\n",
      "Test accuracy: 0.7576054955839058\n",
      "Epoch 5, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4603488505178644\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.5730392156101186\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 6, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4542457540799005\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.6150738569424361\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 7, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44688512022359084\n",
      "Training accuracy: 0.8241843892662287\n",
      "Test loss: 0.5647371958235646\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 8, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44353508175434825\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.5567435756164866\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 9, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.44516099859831687\n",
      "Training accuracy: 0.8273020821734773\n",
      "Test loss: 0.5153443340113399\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 10, lr: 0.004, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4488249152795919\n",
      "Training accuracy: 0.8251865048435586\n",
      "Test loss: 0.6345413299725264\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 1, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.536355276559618\n",
      "Training accuracy: 0.7906691905133059\n",
      "Test loss: 0.7557003106172466\n",
      "Test accuracy: 0.6957801766437685\n",
      "Epoch 2, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4958221011578061\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.5305843453879914\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 3, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4837197103979274\n",
      "Training accuracy: 0.8114909252867164\n",
      "Test loss: 0.531880372815324\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 4, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4659822680227821\n",
      "Training accuracy: 0.8189511190290614\n",
      "Test loss: 0.5546225376054279\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 5, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4690952171741516\n",
      "Training accuracy: 0.820175926956909\n",
      "Test loss: 0.5462996744431036\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 6, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4669389168505493\n",
      "Training accuracy: 0.8196191960806146\n",
      "Test loss: 0.5514303656684287\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 7, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.46117842467533904\n",
      "Training accuracy: 0.8199532346063912\n",
      "Test loss: 0.5300804649765756\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 8, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.45872571308142146\n",
      "Training accuracy: 0.825743235719853\n",
      "Test loss: 0.5757242781608215\n",
      "Test accuracy: 0.7723258096172718\n",
      "Epoch 9, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.45516640232939615\n",
      "Training accuracy: 0.8235163122146755\n",
      "Test loss: 0.6970490878767776\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 10, lr: 0.005, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4611755014594129\n",
      "Training accuracy: 0.8218461195857922\n",
      "Test loss: 0.5642876392842743\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 1, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5547804486683862\n",
      "Training accuracy: 0.788664959358646\n",
      "Test loss: 0.5162382548468387\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 2, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5110438964956522\n",
      "Training accuracy: 0.8030286159670416\n",
      "Test loss: 0.5715566381037059\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 3, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4958567794898931\n",
      "Training accuracy: 0.8123816946887874\n",
      "Test loss: 0.6214876435221821\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 4, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.48933295938607885\n",
      "Training accuracy: 0.811824963812493\n",
      "Test loss: 0.5492080667652958\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 5, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4801762233849239\n",
      "Training accuracy: 0.8153880414207771\n",
      "Test loss: 0.5258285829082673\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 6, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4725390175192994\n",
      "Training accuracy: 0.8187284266785436\n",
      "Test loss: 0.5668469883274868\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 7, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.47482917817447834\n",
      "Training accuracy: 0.8195078499053557\n",
      "Test loss: 0.6329694203118463\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 8, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4805123953335589\n",
      "Training accuracy: 0.8185057343280259\n",
      "Test loss: 0.7321810759786769\n",
      "Test accuracy: 0.6849852796859667\n",
      "Epoch 9, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.46926367816793907\n",
      "Training accuracy: 0.821178042534239\n",
      "Test loss: 0.6015206303399959\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 10, lr: 0.006, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4833583213607103\n",
      "Training accuracy: 0.8153880414207771\n",
      "Test loss: 0.5712112672371065\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 1, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5877679280920955\n",
      "Training accuracy: 0.7773076494822403\n",
      "Test loss: 0.6671902425038803\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 2, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5116123449575422\n",
      "Training accuracy: 0.8060349626990313\n",
      "Test loss: 0.5799501564598645\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.514875532789414\n",
      "Training accuracy: 0.8060349626990313\n",
      "Test loss: 0.6436496646645259\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 4, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.49658350919763966\n",
      "Training accuracy: 0.809932078833092\n",
      "Test loss: 0.5773424433070848\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 5, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5002363552867722\n",
      "Training accuracy: 0.8111568867609398\n",
      "Test loss: 0.6694950742155342\n",
      "Test accuracy: 0.7438665358194309\n",
      "Epoch 6, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4821424668289986\n",
      "Training accuracy: 0.815610733771295\n",
      "Test loss: 0.5304500177517255\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 7, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4968412605542248\n",
      "Training accuracy: 0.8114909252867164\n",
      "Test loss: 0.710386508580396\n",
      "Test accuracy: 0.7507360157016683\n",
      "Epoch 8, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.49641073215327675\n",
      "Training accuracy: 0.8123816946887874\n",
      "Test loss: 0.6083012124390083\n",
      "Test accuracy: 0.7625122669283612\n",
      "Epoch 9, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.49079417858499685\n",
      "Training accuracy: 0.8182830419775081\n",
      "Test loss: 0.539523220243585\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 10, lr: 0.007, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.4954786680845359\n",
      "Training accuracy: 0.8129384255650819\n",
      "Test loss: 0.5442651811479001\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 1, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.6010532780872822\n",
      "Training accuracy: 0.7795345729874179\n",
      "Test loss: 0.7372170961640183\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 2, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.55183300758547\n",
      "Training accuracy: 0.7962364992762498\n",
      "Test loss: 0.8677813904442193\n",
      "Test accuracy: 0.7409224730127576\n",
      "Epoch 3, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.537015602266373\n",
      "Training accuracy: 0.8019151542144527\n",
      "Test loss: 0.5602945806175266\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 4, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.536521118781758\n",
      "Training accuracy: 0.8012470771628994\n",
      "Test loss: 0.7452880773506876\n",
      "Test accuracy: 0.7487733071638861\n",
      "Epoch 5, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5133821097238997\n",
      "Training accuracy: 0.8092640017815388\n",
      "Test loss: 0.6570704515595666\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 6, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5075911374217403\n",
      "Training accuracy: 0.8106001558846454\n",
      "Test loss: 0.5526872000001715\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 7, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5064918513938348\n",
      "Training accuracy: 0.8139405411424118\n",
      "Test loss: 0.7487221486552073\n",
      "Test accuracy: 0.7242394504416094\n",
      "Epoch 8, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5306297695234562\n",
      "Training accuracy: 0.8064803474000668\n",
      "Test loss: 0.6113970413058265\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 9, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5060019722535448\n",
      "Training accuracy: 0.8123816946887874\n",
      "Test loss: 0.8637735323068321\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 10, lr: 0.008, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5009379008739885\n",
      "Training accuracy: 0.8144972720187061\n",
      "Test loss: 1.5261874182535458\n",
      "Test accuracy: 0.7085377821393523\n",
      "Epoch 1, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.6239617831350475\n",
      "Training accuracy: 0.777530341832758\n",
      "Test loss: 1.3004103800148445\n",
      "Test accuracy: 0.7085377821393523\n",
      "Epoch 2, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5570388983251947\n",
      "Training accuracy: 0.7946776528226256\n",
      "Test loss: 1.4128763596138847\n",
      "Test accuracy: 0.5583905789990187\n",
      "Epoch 3, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5537244275688474\n",
      "Training accuracy: 0.8006903462866051\n",
      "Test loss: 1.43030877272444\n",
      "Test accuracy: 0.5623159960745829\n",
      "Epoch 4, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.52472769907463\n",
      "Training accuracy: 0.8043647700701481\n",
      "Test loss: 0.5875796414802072\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 5, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5231935482900666\n",
      "Training accuracy: 0.8072597706268789\n",
      "Test loss: 0.8187274756796578\n",
      "Test accuracy: 0.7507360157016683\n",
      "Epoch 6, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5238202379429716\n",
      "Training accuracy: 0.809932078833092\n",
      "Test loss: 0.656052574851679\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 7, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5315199185753039\n",
      "Training accuracy: 0.8077051553279145\n",
      "Test loss: 0.527508331205005\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 8, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5250762780277332\n",
      "Training accuracy: 0.8069257321011023\n",
      "Test loss: 0.5716077416872019\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 9, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5248020552957381\n",
      "Training accuracy: 0.8092640017815388\n",
      "Test loss: 0.6569063548835852\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 10, lr: 0.009, gamma: 0.07\n",
      "-------------------------------\n",
      "Training loss: 0.5231644083617087\n",
      "Training accuracy: 0.8120476561630108\n",
      "Test loss: 0.8139418921596988\n",
      "Test accuracy: 0.7438665358194309\n",
      "Epoch 1, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5651408999232483\n",
      "Training accuracy: 0.7755261106780982\n",
      "Test loss: 0.5481459561870657\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 2, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4904840256864915\n",
      "Training accuracy: 0.809932078833092\n",
      "Test loss: 0.5348550240939686\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 3, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4742704046527815\n",
      "Training accuracy: 0.8154993875960361\n",
      "Test loss: 0.5354699439810583\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 4, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4646070644951334\n",
      "Training accuracy: 0.8199532346063912\n",
      "Test loss: 0.5159688856346685\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 5, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4591473422022867\n",
      "Training accuracy: 0.8230709275136399\n",
      "Test loss: 0.5283144214085438\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 6, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4541232576681901\n",
      "Training accuracy: 0.8230709275136399\n",
      "Test loss: 0.5097661401210051\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 7, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4499741630045293\n",
      "Training accuracy: 0.8278588130497717\n",
      "Test loss: 0.5130380863237896\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 8, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4462793913726193\n",
      "Training accuracy: 0.8274134283487362\n",
      "Test loss: 0.5089791641151121\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 9, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.44523036173332137\n",
      "Training accuracy: 0.8301970827302082\n",
      "Test loss: 0.516979742623405\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 10, lr: 0.001, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4419161778703695\n",
      "Training accuracy: 0.8299743903796905\n",
      "Test loss: 0.5131096101134752\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 1, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5341052379935745\n",
      "Training accuracy: 0.7882195746576105\n",
      "Test loss: 0.6196210041598319\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 2, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.47946925914748156\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.5290369241113635\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 3, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.46485598176110493\n",
      "Training accuracy: 0.8182830419775081\n",
      "Test loss: 0.5180355137299977\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 4, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.45796684003091315\n",
      "Training accuracy: 0.8191738113795791\n",
      "Test loss: 0.5342583824186727\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 5, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.45208519644095335\n",
      "Training accuracy: 0.8266340051219241\n",
      "Test loss: 0.5223085337169505\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 6, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4474886043866033\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5753447872031776\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 7, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4458567983634806\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.5744415134688706\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 8, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4410104165628325\n",
      "Training accuracy: 0.8307538136065026\n",
      "Test loss: 0.512169530628006\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 9, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4370093570627889\n",
      "Training accuracy: 0.8307538136065026\n",
      "Test loss: 0.5147768821168811\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 10, lr: 0.002, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.43530195703911045\n",
      "Training accuracy: 0.8290836209776195\n",
      "Test loss: 0.520051575561033\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 1, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5295792841098977\n",
      "Training accuracy: 0.7947889989978845\n",
      "Test loss: 0.6351863882843481\n",
      "Test accuracy: 0.732090284592738\n",
      "Epoch 2, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4806434912865285\n",
      "Training accuracy: 0.8147199643692239\n",
      "Test loss: 0.5988346071542772\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 3, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4694271894758149\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.5229076564136501\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 4, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4603053322614613\n",
      "Training accuracy: 0.8207326578332035\n",
      "Test loss: 0.5641881389519651\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 5, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4532786409867631\n",
      "Training accuracy: 0.8245184277920053\n",
      "Test loss: 0.5874056884538204\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 6, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4511980788205464\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.579567200126779\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 7, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4470563790053809\n",
      "Training accuracy: 0.8264113127714063\n",
      "Test loss: 0.5212902881797327\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 8, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4429255126977654\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.5072104832901454\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 9, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.44149459792781603\n",
      "Training accuracy: 0.8266340051219241\n",
      "Test loss: 0.5469051118687863\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 10, lr: 0.003, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4427883264655035\n",
      "Training accuracy: 0.8273020821734773\n",
      "Test loss: 0.5204252982630931\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 1, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5324334147751377\n",
      "Training accuracy: 0.7908918828638236\n",
      "Test loss: 0.6430531473459276\n",
      "Test accuracy: 0.7242394504416094\n",
      "Epoch 2, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.49043787685332324\n",
      "Training accuracy: 0.8093753479567977\n",
      "Test loss: 0.6084983990517636\n",
      "Test accuracy: 0.7703631010794897\n",
      "Epoch 3, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.47348004128742877\n",
      "Training accuracy: 0.8188397728538025\n",
      "Test loss: 0.5693068868746585\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 4, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4677604381789371\n",
      "Training accuracy: 0.8176149649259548\n",
      "Test loss: 0.5099955897511397\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 5, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4611367729316508\n",
      "Training accuracy: 0.8225141966373455\n",
      "Test loss: 0.5292897555205726\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 6, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4619404363839269\n",
      "Training accuracy: 0.8190624652043202\n",
      "Test loss: 0.6180972889197353\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 7, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4548139728107161\n",
      "Training accuracy: 0.8238503507404521\n",
      "Test loss: 0.5208539222828656\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 8, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4493219913173765\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.5161074891584077\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 9, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.45314950053204195\n",
      "Training accuracy: 0.8249638124930408\n",
      "Test loss: 0.6731651869910505\n",
      "Test accuracy: 0.7664376840039254\n",
      "Epoch 10, lr: 0.004, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4481432299852345\n",
      "Training accuracy: 0.8259659280703708\n",
      "Test loss: 0.5588654640319888\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 1, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5323641827341815\n",
      "Training accuracy: 0.7913372675648591\n",
      "Test loss: 0.6154732276576775\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 2, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.48801894634447734\n",
      "Training accuracy: 0.8112682329361987\n",
      "Test loss: 0.5199268717059209\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 3, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.48374346207568913\n",
      "Training accuracy: 0.8150540028950005\n",
      "Test loss: 0.6091904305908701\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 4, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.47020338769214426\n",
      "Training accuracy: 0.8200645807816501\n",
      "Test loss: 0.521794098574701\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 5, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.47186226206863263\n",
      "Training accuracy: 0.8196191960806146\n",
      "Test loss: 0.6941202014713924\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 6, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.47219615032513335\n",
      "Training accuracy: 0.8186170805032847\n",
      "Test loss: 0.6017509002797854\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 7, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4588882465223708\n",
      "Training accuracy: 0.8231822736888987\n",
      "Test loss: 0.6587604614188557\n",
      "Test accuracy: 0.7301275760549558\n",
      "Epoch 8, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.46320115209833684\n",
      "Training accuracy: 0.8181716958022492\n",
      "Test loss: 0.5617996675928395\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 9, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.46489780178288814\n",
      "Training accuracy: 0.8196191960806146\n",
      "Test loss: 0.5246925397288928\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 10, lr: 0.005, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4598377729435167\n",
      "Training accuracy: 0.8225141966373455\n",
      "Test loss: 0.5382145937747881\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 1, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5630228970274072\n",
      "Training accuracy: 0.7790891882863824\n",
      "Test loss: 0.5618324768788915\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 2, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5098113724707813\n",
      "Training accuracy: 0.8023605389154883\n",
      "Test loss: 0.5356616873853457\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 3, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5058962598753299\n",
      "Training accuracy: 0.8059236165237724\n",
      "Test loss: 0.5286217561764853\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 4, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4914145453604594\n",
      "Training accuracy: 0.8116022714619753\n",
      "Test loss: 0.5845321381758894\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 5, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4907411085366008\n",
      "Training accuracy: 0.8129384255650819\n",
      "Test loss: 0.5649725264843125\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 6, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4705639770309453\n",
      "Training accuracy: 0.819285157554838\n",
      "Test loss: 0.7067765741516727\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 7, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4738863173890069\n",
      "Training accuracy: 0.8175036187506959\n",
      "Test loss: 0.5505684070779017\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 8, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4712796843102682\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.6062733362537605\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 9, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.46822696402561104\n",
      "Training accuracy: 0.8217347734105334\n",
      "Test loss: 0.6984488978704129\n",
      "Test accuracy: 0.7428851815505397\n",
      "Epoch 10, lr: 0.006, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4783981906459799\n",
      "Training accuracy: 0.8207326578332035\n",
      "Test loss: 0.5415634013661225\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 1, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5698524416388973\n",
      "Training accuracy: 0.7817614964925955\n",
      "Test loss: 0.8783297751672836\n",
      "Test accuracy: 0.7232580961727183\n",
      "Epoch 2, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5138030171912015\n",
      "Training accuracy: 0.8072597706268789\n",
      "Test loss: 0.8843700271593343\n",
      "Test accuracy: 0.6830225711481845\n",
      "Epoch 3, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5168108734186183\n",
      "Training accuracy: 0.8024718850907472\n",
      "Test loss: 0.5514772613921273\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 4, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.49430953868313365\n",
      "Training accuracy: 0.8102661173588688\n",
      "Test loss: 0.6335879906470923\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 5, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.49202545080613935\n",
      "Training accuracy: 0.8120476561630108\n",
      "Test loss: 0.6810595360424615\n",
      "Test accuracy: 0.719332679097154\n",
      "Epoch 6, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.49845610740999513\n",
      "Training accuracy: 0.8098207326578332\n",
      "Test loss: 0.6387220075366775\n",
      "Test accuracy: 0.7595682041216879\n",
      "Epoch 7, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5032899522884905\n",
      "Training accuracy: 0.8078165015031734\n",
      "Test loss: 0.6428016921079428\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 8, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4867017992872024\n",
      "Training accuracy: 0.8152766952455183\n",
      "Test loss: 0.5503216936263065\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 9, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4925259963904095\n",
      "Training accuracy: 0.8150540028950005\n",
      "Test loss: 0.5247020497991246\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 10, lr: 0.007, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.4892250966314297\n",
      "Training accuracy: 0.8157220799465539\n",
      "Test loss: 0.7034414272547003\n",
      "Test accuracy: 0.7222767419038273\n",
      "Epoch 1, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5881815042743677\n",
      "Training accuracy: 0.7796459191626768\n",
      "Test loss: 1.289489493519799\n",
      "Test accuracy: 0.7124631992149166\n",
      "Epoch 2, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5490985493287784\n",
      "Training accuracy: 0.7987974613072041\n",
      "Test loss: 0.6249497758628576\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 3, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5253453385460258\n",
      "Training accuracy: 0.8063690012248079\n",
      "Test loss: 0.5847658530184753\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 4, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.548975560091981\n",
      "Training accuracy: 0.8023605389154883\n",
      "Test loss: 0.8007261923882631\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 5, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.541669268429432\n",
      "Training accuracy: 0.7992428460082396\n",
      "Test loss: 0.5450318644044444\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 6, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5111254650704142\n",
      "Training accuracy: 0.8134951564413763\n",
      "Test loss: 0.6897619174202946\n",
      "Test accuracy: 0.7850834151128557\n",
      "Epoch 7, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.501210745538931\n",
      "Training accuracy: 0.814608618193965\n",
      "Test loss: 0.5798961637766485\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 8, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.49160755901131725\n",
      "Training accuracy: 0.8175036187506959\n",
      "Test loss: 0.6643678027701448\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 9, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5215306514429231\n",
      "Training accuracy: 0.8044761162454069\n",
      "Test loss: 0.5509579472382707\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 10, lr: 0.008, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.50676674448147\n",
      "Training accuracy: 0.8134951564413763\n",
      "Test loss: 0.9585487199133816\n",
      "Test accuracy: 0.6486751717369971\n",
      "Epoch 1, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.6256893199783778\n",
      "Training accuracy: 0.775637456853357\n",
      "Test loss: 0.5450456324339615\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 2, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5424451940960402\n",
      "Training accuracy: 0.8031399621423004\n",
      "Test loss: 0.6902721874613289\n",
      "Test accuracy: 0.7438665358194309\n",
      "Epoch 3, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5603406170656654\n",
      "Training accuracy: 0.7982407304309097\n",
      "Test loss: 0.860573761830503\n",
      "Test accuracy: 0.661432777232581\n",
      "Epoch 4, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5534800635740972\n",
      "Training accuracy: 0.7977953457298742\n",
      "Test loss: 0.9311152548036584\n",
      "Test accuracy: 0.7399411187438666\n",
      "Epoch 5, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5187012048092043\n",
      "Training accuracy: 0.8107115020599043\n",
      "Test loss: 0.6394584274853528\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 6, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5431318661773965\n",
      "Training accuracy: 0.8065916935753257\n",
      "Test loss: 0.6840714434412206\n",
      "Test accuracy: 0.76153091265947\n",
      "Epoch 7, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5472770555023891\n",
      "Training accuracy: 0.801692461863935\n",
      "Test loss: 0.7164609366123061\n",
      "Test accuracy: 0.7369970559371933\n",
      "Epoch 8, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5498550586214098\n",
      "Training accuracy: 0.8059236165237724\n",
      "Test loss: 0.6269538195491188\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 9, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5334232181141416\n",
      "Training accuracy: 0.80815054002895\n",
      "Test loss: 1.4846367514285537\n",
      "Test accuracy: 0.7085377821393523\n",
      "Epoch 10, lr: 0.009, gamma: 0.08\n",
      "-------------------------------\n",
      "Training loss: 0.5559723263075697\n",
      "Training accuracy: 0.8032513083175593\n",
      "Test loss: 0.669165218835724\n",
      "Test accuracy: 0.732090284592738\n",
      "Epoch 1, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5754173053902436\n",
      "Training accuracy: 0.7731878409976617\n",
      "Test loss: 0.554974535964539\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 2, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4979695761297344\n",
      "Training accuracy: 0.8092640017815388\n",
      "Test loss: 0.5419293409240843\n",
      "Test accuracy: 0.7752698724239451\n",
      "Epoch 3, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.48089996346279473\n",
      "Training accuracy: 0.8147199643692239\n",
      "Test loss: 0.5299030077937072\n",
      "Test accuracy: 0.7929342492639843\n",
      "Epoch 4, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4695584989048009\n",
      "Training accuracy: 0.8191738113795791\n",
      "Test loss: 0.5179619046258973\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 5, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.46236317497015983\n",
      "Training accuracy: 0.8234049660394166\n",
      "Test loss: 0.5343867633656267\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 6, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.456503853820743\n",
      "Training accuracy: 0.8251865048435586\n",
      "Test loss: 0.511648083049018\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 7, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4536127705090084\n",
      "Training accuracy: 0.8262999665961475\n",
      "Test loss: 0.5325917219624318\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 8, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4477989596097271\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.504540432745388\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 9, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44540957418452815\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.535039447269917\n",
      "Test accuracy: 0.7772325809617272\n",
      "Epoch 10, lr: 0.001, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4442434255098025\n",
      "Training accuracy: 0.8300857365549493\n",
      "Test loss: 0.5085601915145646\n",
      "Test accuracy: 0.7939156035328754\n",
      "Epoch 1, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5393646966905321\n",
      "Training accuracy: 0.7932301525442601\n",
      "Test loss: 0.5820926029209533\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 2, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4856788357359943\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 0.5423761386281726\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 3, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4680323921655868\n",
      "Training accuracy: 0.8212893887094979\n",
      "Test loss: 0.5294912652833655\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 4, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4625163479765066\n",
      "Training accuracy: 0.8225141966373455\n",
      "Test loss: 0.5279769289130902\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 5, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.45299210093849923\n",
      "Training accuracy: 0.8237390045651932\n",
      "Test loss: 0.5644464027776335\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 6, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.45067120508950664\n",
      "Training accuracy: 0.8251865048435586\n",
      "Test loss: 0.5878614057970468\n",
      "Test accuracy: 0.7634936211972522\n",
      "Epoch 7, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4463391587139835\n",
      "Training accuracy: 0.828526890101325\n",
      "Test loss: 0.5159038456780168\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 8, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44329192588356126\n",
      "Training accuracy: 0.8284155439260661\n",
      "Test loss: 0.533414961252877\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 9, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.43976150384234397\n",
      "Training accuracy: 0.8289722748023606\n",
      "Test loss: 0.5205914395714181\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 10, lr: 0.002, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4367075468235273\n",
      "Training accuracy: 0.8320899677096092\n",
      "Test loss: 0.5094505839188738\n",
      "Test accuracy: 0.7909715407262021\n",
      "Epoch 1, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5280426805951974\n",
      "Training accuracy: 0.7928961140184835\n",
      "Test loss: 0.5546904330281678\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 2, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.48227397998129784\n",
      "Training accuracy: 0.8142745796681884\n",
      "Test loss: 0.5148114022141701\n",
      "Test accuracy: 0.7919528949950932\n",
      "Epoch 3, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4674558376511995\n",
      "Training accuracy: 0.823961696915711\n",
      "Test loss: 0.5270677345189777\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 4, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.46086851944040447\n",
      "Training accuracy: 0.8222915042868277\n",
      "Test loss: 0.5520721595292003\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 5, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.45493154493037663\n",
      "Training accuracy: 0.8268566974724418\n",
      "Test loss: 0.5171246638592841\n",
      "Test accuracy: 0.7831207065750736\n",
      "Epoch 6, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44764258273923974\n",
      "Training accuracy: 0.8313105444827971\n",
      "Test loss: 0.5372915425876172\n",
      "Test accuracy: 0.7860647693817469\n",
      "Epoch 7, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44816607246983226\n",
      "Training accuracy: 0.826745351297183\n",
      "Test loss: 0.5132283238772672\n",
      "Test accuracy: 0.7948969578017664\n",
      "Epoch 8, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44445260658705904\n",
      "Training accuracy: 0.8275247745239951\n",
      "Test loss: 0.5831441215958749\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 9, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4436442911286434\n",
      "Training accuracy: 0.8290836209776195\n",
      "Test loss: 0.640291904970287\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 10, lr: 0.003, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44005434687771283\n",
      "Training accuracy: 0.8290836209776195\n",
      "Test loss: 0.5257017620646334\n",
      "Test accuracy: 0.7811579980372915\n",
      "Epoch 1, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5419549731498916\n",
      "Training accuracy: 0.7862153435029506\n",
      "Test loss: 0.9114472309787327\n",
      "Test accuracy: 0.6408243375858685\n",
      "Epoch 2, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.49194638862626855\n",
      "Training accuracy: 0.8122703485135285\n",
      "Test loss: 0.5338101067065725\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 3, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4749622657777286\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5158884349526788\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 4, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.46697191250787495\n",
      "Training accuracy: 0.8170582340496604\n",
      "Test loss: 0.5896874338651663\n",
      "Test accuracy: 0.7556427870461236\n",
      "Epoch 5, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.46353047847907086\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.6399675032692405\n",
      "Test accuracy: 0.7536800785083415\n",
      "Epoch 6, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.45734728160155086\n",
      "Training accuracy: 0.8227368889878632\n",
      "Test loss: 0.5140367721493508\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 7, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4558554385469242\n",
      "Training accuracy: 0.8230709275136399\n",
      "Test loss: 0.5224051669722567\n",
      "Test accuracy: 0.7968596663395485\n",
      "Epoch 8, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.45043222138789035\n",
      "Training accuracy: 0.8241843892662287\n",
      "Test loss: 0.6566901354373261\n",
      "Test accuracy: 0.7252208047105005\n",
      "Epoch 9, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4514282639879331\n",
      "Training accuracy: 0.8269680436477007\n",
      "Test loss: 0.5396225398609285\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 10, lr: 0.004, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.44885899020507625\n",
      "Training accuracy: 0.828638236276584\n",
      "Test loss: 0.6242715945070695\n",
      "Test accuracy: 0.7477919528949951\n",
      "Epoch 1, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5445558527651907\n",
      "Training accuracy: 0.7906691905133059\n",
      "Test loss: 0.7472925162759918\n",
      "Test accuracy: 0.6849852796859667\n",
      "Epoch 2, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.49556888411124983\n",
      "Training accuracy: 0.8083732323794678\n",
      "Test loss: 0.5208665183704196\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 3, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.47971859510936976\n",
      "Training accuracy: 0.8158334261218128\n",
      "Test loss: 0.551199139158906\n",
      "Test accuracy: 0.7762512266928361\n",
      "Epoch 4, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.47842110495513696\n",
      "Training accuracy: 0.8124930408640463\n",
      "Test loss: 1.0493429021231677\n",
      "Test accuracy: 0.6064769381746811\n",
      "Epoch 5, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4740518091974247\n",
      "Training accuracy: 0.8195078499053557\n",
      "Test loss: 0.5287922826962101\n",
      "Test accuracy: 0.7841020608439647\n",
      "Epoch 6, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4691588382667322\n",
      "Training accuracy: 0.8175036187506959\n",
      "Test loss: 0.6142866517306544\n",
      "Test accuracy: 0.7674190382728164\n",
      "Epoch 7, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.46819445896342476\n",
      "Training accuracy: 0.8209553501837212\n",
      "Test loss: 0.5727265633357519\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 8, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4664230235680057\n",
      "Training accuracy: 0.8217347734105334\n",
      "Test loss: 0.5314565323753844\n",
      "Test accuracy: 0.78900883218842\n",
      "Epoch 9, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4540958799875575\n",
      "Training accuracy: 0.8258545818951119\n",
      "Test loss: 0.770399920020885\n",
      "Test accuracy: 0.7566241413150148\n",
      "Epoch 10, lr: 0.005, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4591020468467727\n",
      "Training accuracy: 0.8231822736888987\n",
      "Test loss: 0.5373302397479954\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 1, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5633154087097976\n",
      "Training accuracy: 0.7834316891214786\n",
      "Test loss: 0.5415698096843417\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 2, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5187116131234601\n",
      "Training accuracy: 0.8041420777196303\n",
      "Test loss: 0.5151094753328086\n",
      "Test accuracy: 0.7899901864573111\n",
      "Epoch 3, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.502440884681152\n",
      "Training accuracy: 0.8075938091526557\n",
      "Test loss: 0.5565011487858803\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 4, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.47743572007174906\n",
      "Training accuracy: 0.8171695802249193\n",
      "Test loss: 0.945116642997824\n",
      "Test accuracy: 0.7301275760549558\n",
      "Epoch 5, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4788268791741245\n",
      "Training accuracy: 0.818394388152767\n",
      "Test loss: 0.5495903553391813\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 6, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4753852504197798\n",
      "Training accuracy: 0.819285157554838\n",
      "Test loss: 0.7647100979493341\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 7, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4774480682049333\n",
      "Training accuracy: 0.8169468878744015\n",
      "Test loss: 0.6017935074229703\n",
      "Test accuracy: 0.7733071638861629\n",
      "Epoch 8, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.48464627134721644\n",
      "Training accuracy: 0.8143859258434473\n",
      "Test loss: 0.5184171108841545\n",
      "Test accuracy: 0.7988223748773308\n",
      "Epoch 9, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.47914482094824495\n",
      "Training accuracy: 0.8180603496269903\n",
      "Test loss: 0.5951357194907064\n",
      "Test accuracy: 0.7782139352306182\n",
      "Epoch 10, lr: 0.006, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.473581026443566\n",
      "Training accuracy: 0.8181716958022492\n",
      "Test loss: 0.646377520659487\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 1, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5575252740848518\n",
      "Training accuracy: 0.7853245741008796\n",
      "Test loss: 0.8328927812679242\n",
      "Test accuracy: 0.7212953876349362\n",
      "Epoch 2, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5331402501899593\n",
      "Training accuracy: 0.7990201536577218\n",
      "Test loss: 0.8486823415030909\n",
      "Test accuracy: 0.7222767419038273\n",
      "Epoch 3, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5077934611375926\n",
      "Training accuracy: 0.8072597706268789\n",
      "Test loss: 0.5223682249025227\n",
      "Test accuracy: 0.7958783120706575\n",
      "Epoch 4, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5041026954176484\n",
      "Training accuracy: 0.8154993875960361\n",
      "Test loss: 0.5794031574400882\n",
      "Test accuracy: 0.7821393523061825\n",
      "Epoch 5, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4992822773431885\n",
      "Training accuracy: 0.8104888097093865\n",
      "Test loss: 0.7926742131506496\n",
      "Test accuracy: 0.6859666339548577\n",
      "Epoch 6, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.49394445533928183\n",
      "Training accuracy: 0.8104888097093865\n",
      "Test loss: 0.5702679991663614\n",
      "Test accuracy: 0.788027477919529\n",
      "Epoch 7, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.503265372629806\n",
      "Training accuracy: 0.8085959247299855\n",
      "Test loss: 0.7485841231619879\n",
      "Test accuracy: 0.7448478900883219\n",
      "Epoch 8, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4958990552273801\n",
      "Training accuracy: 0.8159447722970716\n",
      "Test loss: 0.5857753597485071\n",
      "Test accuracy: 0.774288518155054\n",
      "Epoch 9, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.49463751852718096\n",
      "Training accuracy: 0.8136065026166351\n",
      "Test loss: 0.6969060414672716\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 10, lr: 0.007, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.4927726399525544\n",
      "Training accuracy: 0.8130497717403408\n",
      "Test loss: 0.6588762148088749\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 1, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5938274653083218\n",
      "Training accuracy: 0.7765282262554282\n",
      "Test loss: 0.8479371588527279\n",
      "Test accuracy: 0.7546614327772326\n",
      "Epoch 2, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5369781725471156\n",
      "Training accuracy: 0.8010243848123817\n",
      "Test loss: 0.6892140744010881\n",
      "Test accuracy: 0.7585868498527969\n",
      "Epoch 3, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5358998721989979\n",
      "Training accuracy: 0.8022491927402293\n",
      "Test loss: 1.5868614076749148\n",
      "Test accuracy: 0.6889106967615309\n",
      "Epoch 4, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5321718392108254\n",
      "Training accuracy: 0.8021378465649704\n",
      "Test loss: 0.553071985096178\n",
      "Test accuracy: 0.7801766437684003\n",
      "Epoch 5, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5288010053971259\n",
      "Training accuracy: 0.8019151542144527\n",
      "Test loss: 0.6331655357957939\n",
      "Test accuracy: 0.7507360157016683\n",
      "Epoch 6, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5214032067307225\n",
      "Training accuracy: 0.8068143859258434\n",
      "Test loss: 0.6087831223092908\n",
      "Test accuracy: 0.7791952894995093\n",
      "Epoch 7, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5065058596202412\n",
      "Training accuracy: 0.8106001558846454\n",
      "Test loss: 0.6165598068199869\n",
      "Test accuracy: 0.7684003925417076\n",
      "Epoch 8, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5217444786926166\n",
      "Training accuracy: 0.8097093864825743\n",
      "Test loss: 0.6404775934434616\n",
      "Test accuracy: 0.745829244357213\n",
      "Epoch 9, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5255526862296054\n",
      "Training accuracy: 0.8064803474000668\n",
      "Test loss: 0.705586967161757\n",
      "Test accuracy: 0.7350343473994112\n",
      "Epoch 10, lr: 0.008, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5100845311136929\n",
      "Training accuracy: 0.8139405411424118\n",
      "Test loss: 0.6534221639694012\n",
      "Test accuracy: 0.7389597644749755\n",
      "Epoch 1, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.6321618621551889\n",
      "Training accuracy: 0.7750807259770627\n",
      "Test loss: 0.6242323917893362\n",
      "Test accuracy: 0.7644749754661433\n",
      "Epoch 2, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5740045782731564\n",
      "Training accuracy: 0.7964591916267676\n",
      "Test loss: 0.9769157964418166\n",
      "Test accuracy: 0.6388616290480864\n",
      "Epoch 3, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5609757064074002\n",
      "Training accuracy: 0.800801692461864\n",
      "Test loss: 0.6263114250209311\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 4, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5562411035246074\n",
      "Training accuracy: 0.798908807482463\n",
      "Test loss: 0.5929624785150934\n",
      "Test accuracy: 0.7713444553483808\n",
      "Epoch 5, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5339546045725018\n",
      "Training accuracy: 0.8074824629773967\n",
      "Test loss: 1.1145093542202884\n",
      "Test accuracy: 0.7301275760549558\n",
      "Epoch 6, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5613059771610467\n",
      "Training accuracy: 0.7984634227814275\n",
      "Test loss: 0.6373589324296047\n",
      "Test accuracy: 0.7693817468105987\n",
      "Epoch 7, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5403951154415146\n",
      "Training accuracy: 0.8069257321011023\n",
      "Test loss: 0.570095202522727\n",
      "Test accuracy: 0.7870461236506379\n",
      "Epoch 8, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5445281158489369\n",
      "Training accuracy: 0.8011357309876406\n",
      "Test loss: 0.665015052883384\n",
      "Test accuracy: 0.7654563297350343\n",
      "Epoch 9, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5493728834212775\n",
      "Training accuracy: 0.8067030397505845\n",
      "Test loss: 0.7408540640190844\n",
      "Test accuracy: 0.7526987242394504\n",
      "Epoch 10, lr: 0.009, gamma: 0.09\n",
      "-------------------------------\n",
      "Training loss: 0.5460275828725524\n",
      "Training accuracy: 0.8019151542144527\n",
      "Test loss: 0.6123459347915836\n",
      "Test accuracy: 0.7674190382728164\n",
      "(0.8301970827302082, 0.7998037291462218, 0.002, 0.07)\n"
     ]
    }
   ],
   "source": [
    "result_li = []\n",
    "for gamma in range(1, 10, 1):\n",
    "  for lr in range(1, 10, 1):\n",
    "    # reset the model\n",
    "    model_hypertune = SVM(32*32*3, num_classes=1)\n",
    "    model_hypertune = model_hypertune.to(device)\n",
    "    for epoch in range(10):\n",
    "      print(f\"Epoch {epoch+1}, lr: {lr/1000}, gamma: {gamma/100}\\n-------------------------------\")\n",
    "      train_acc = train_loop(train_set, model_hypertune, float(gamma/100), float(lr/1000))\n",
    "      val_acc = test_loop(val_set, model_hypertune, float(gamma/100))\n",
    "      \n",
    "      if epoch == 9:\n",
    "        result_li.append((train_acc, val_acc, lr/1000, gamma/100))\n",
    "\n",
    "result_li = sorted(result_li, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "print(result_li[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) final test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 0.5372375277224357\n",
      "Training accuracy: 0.7852924791086351\n",
      "Test loss: 0.4605483832359314\n",
      "Test accuracy: 0.813\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 0.48283420436229546\n",
      "Training accuracy: 0.8101392757660167\n",
      "Test loss: 0.43905397891998293\n",
      "Test accuracy: 0.825\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 0.46888236003665873\n",
      "Training accuracy: 0.8147075208913649\n",
      "Test loss: 0.4639766826629639\n",
      "Test accuracy: 0.8065\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 0.4586989210575072\n",
      "Training accuracy: 0.8187186629526463\n",
      "Test loss: 0.4346914262771606\n",
      "Test accuracy: 0.8205\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 0.45378209390348045\n",
      "Training accuracy: 0.8229526462395543\n",
      "Test loss: 0.42124662244319916\n",
      "Test accuracy: 0.829\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 0.45118903174373765\n",
      "Training accuracy: 0.8271866295264624\n",
      "Test loss: 0.44221025657653806\n",
      "Test accuracy: 0.8155\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 0.44888551544678246\n",
      "Training accuracy: 0.8222841225626741\n",
      "Test loss: 0.4319095897674561\n",
      "Test accuracy: 0.8195\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 0.44495588463993124\n",
      "Training accuracy: 0.8259610027855153\n",
      "Test loss: 0.4168991763591766\n",
      "Test accuracy: 0.832\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 0.44294133930817287\n",
      "Training accuracy: 0.8277437325905292\n",
      "Test loss: 0.4411657235622406\n",
      "Test accuracy: 0.8175\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 0.4408434057999454\n",
      "Training accuracy: 0.8250696378830084\n",
      "Test loss: 0.4243371186256409\n",
      "Test accuracy: 0.8255\n"
     ]
    }
   ],
   "source": [
    "final_model_binary_SVM = SVM(32*32*3, num_classes=1)\n",
    "final_model_binary_SVM = final_model_binary_SVM.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  train_acc = train_loop(train_set, final_model_binary_SVM, gamma=0.07, lr = 0.002)\n",
    "  test_acc = test_loop(testset, final_model_binary_SVM, gamma=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Perform multiclass classification using multiple one vs all soft margin SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set size:  45000\n",
      "Validation set size:  5000\n",
      "Test set size:  10000\n"
     ]
    }
   ],
   "source": [
    "# Re-load the dataset and Normalize\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_set, val_set = split_train_val(trainset)\n",
    "print(\"Train set size: \", len(train_set))\n",
    "print(\"Validation set size: \", len(val_set))\n",
    "print(\"Test set size: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_set = DataLoader(val_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "testset = DataLoader(testset, batch_size=64, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hinge_Multi_SVM(y_pred, y_true, gamma, model, class_num):\n",
    "  # if y_true is i, convert to 1\n",
    "  # else convert to -1\n",
    "  y_gt = torch.where(y_true == class_num, 1, -1)\n",
    "  # Ensure dimensions match\n",
    "  y_gt = y_gt.unsqueeze(1)\n",
    "  # Calculate hinge loss for each sample\n",
    "  res = torch.where(y_pred > 0, 1, -1)\n",
    "  correct = correct_pred(res, y_gt)\n",
    "  losses = torch.max(torch.zeros(y_pred.shape), 1 - y_pred * y_gt)\n",
    "  losses += gamma * torch.sum(model.layer1.weight ** 2)\n",
    "  \n",
    "  return losses.mean(), correct, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_loop_Multi_SVM(dataloader, model, gamma, lr, class_num):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  train_tens = torch.zeros((len(dataloader.dataset), 1))\n",
    "\n",
    "  optim = optimizer(model, lr)\n",
    "  \n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss, correct, res = loss_hinge_Multi_SVM(pred, y, gamma, model, class_num)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "  \n",
    "    total_loss += loss.item() * X.size(0)\n",
    "    total_correct += correct\n",
    "\n",
    "    for i in range(len(res)):\n",
    "      sample_idx = batch * dataloader.batch_size + i\n",
    "      train_tens[sample_idx] = res[i]\n",
    "    \n",
    "  print(f\"Training loss: {total_loss / size}\")\n",
    "  #print(f\"Training accuracy: {total_correct / size}\")\n",
    "  return pred, total_correct, total_loss, train_tens\n",
    "\n",
    "# Test the model\n",
    "def test_loop_Multi_SVM(dataloader, models, train_tens, gamma):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_correct = 0\n",
    "  result_tens = torch.zeros((len(dataloader.dataset), 9))  # 2D tensor for storing predictions across classes\n",
    "\n",
    "  for class_num in range(9): \n",
    "    model_ = models[class_num]\n",
    "    model_.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_gt = torch.where(y == class_num, 1, -1).to(device)\n",
    "        \n",
    "        pred = model_(X)\n",
    "        loss, correct, res = loss_hinge_Multi_SVM(pred, y_gt, gamma, model_, class_num)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        for i in range(len(res)):\n",
    "          sample_idx = batch_idx * dataloader.batch_size + i\n",
    "          result_tens[sample_idx, class_num] = res[i]\n",
    "\n",
    "    print(f\"Test loss for class {class_num}: {total_loss / size}\")\n",
    "\n",
    "  # accuracy calculation\n",
    "  for i in range(len(result_tens)): \n",
    "    res_ = result_tens[i]\n",
    "    if (res_ == -1).all() or ((res_ == 1).sum() == 1 and (res_ == -1).sum() == len(res_) - 1):\n",
    "      if train_tens[i] == 1:\n",
    "        total_correct += 1\n",
    "\n",
    "  #print(\"Result Tensor:\", result_tens) \n",
    "  return total_correct, result_tens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.7332859108395047\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.8067486012352837\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.7651013175116645\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.7409722497728136\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.783869953918457\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.7107228898366292\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.7515205088297526\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.7858535222371419\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.7468806915283203\n",
      "Test loss for class 0: 0.6889760694503784\n",
      "Test loss for class 1: 0.7084241714477539\n",
      "Test loss for class 2: 0.6721014846801758\n",
      "Test loss for class 3: 0.6460918907165527\n",
      "Test loss for class 4: 0.6859098178863525\n",
      "Test loss for class 5: 0.6614056589126587\n",
      "Test loss for class 6: 0.6716500057220459\n",
      "Test loss for class 7: 0.6656892684936524\n",
      "Test loss for class 8: 0.722409716796875\n",
      "Overall Accuracy after Epoch 1: 0.08\n",
      " Epoch 2\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.6310669120788575\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.6731343414306641\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.6576690019819471\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.6271431460062663\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.6872579453786214\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.6186232675764296\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.6598225240071615\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.7016685007731119\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.65774547229343\n",
      "Test loss for class 0: 0.6271199281692504\n",
      "Test loss for class 1: 0.6377577669143677\n",
      "Test loss for class 2: 0.6028902278900147\n",
      "Test loss for class 3: 0.5742743816375733\n",
      "Test loss for class 4: 0.6220762294769288\n",
      "Test loss for class 5: 0.5951346805572509\n",
      "Test loss for class 6: 0.600978402709961\n",
      "Test loss for class 7: 0.5900576593399048\n",
      "Test loss for class 8: 0.6621518594741821\n",
      "Overall Accuracy after Epoch 2: 0.09\n",
      " Epoch 3\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.5922087978786892\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.6232101426018609\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.6154053396013048\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.5829322410159641\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.6505008085038927\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.5817674646589491\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.6162812845018175\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.6607375764211019\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.6221327406989203\n",
      "Test loss for class 0: 0.5903108222961426\n",
      "Test loss for class 1: 0.6005551067352295\n",
      "Test loss for class 2: 0.560150520324707\n",
      "Test loss for class 3: 0.5323588832855225\n",
      "Test loss for class 4: 0.5778962944030762\n",
      "Test loss for class 5: 0.5573238554000854\n",
      "Test loss for class 6: 0.5560166090011597\n",
      "Test loss for class 7: 0.5446921674728393\n",
      "Test loss for class 8: 0.6215517896652222\n",
      "Overall Accuracy after Epoch 3: 0.09\n",
      " Epoch 4\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.5650860276963976\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.5921440643734402\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.587476288011339\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.5558498204125298\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.6239112152523465\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.5584469409094917\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.5840929965549045\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.6307958341810438\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5962023119184706\n",
      "Test loss for class 0: 0.5511918674468994\n",
      "Test loss for class 1: 0.5737093004226684\n",
      "Test loss for class 2: 0.5263677206039429\n",
      "Test loss for class 3: 0.5036152318954468\n",
      "Test loss for class 4: 0.544880792427063\n",
      "Test loss for class 5: 0.5267262849807739\n",
      "Test loss for class 6: 0.5285100589752197\n",
      "Test loss for class 7: 0.5090434844970703\n",
      "Test loss for class 8: 0.5797738920211792\n",
      "Overall Accuracy after Epoch 4: 0.10\n",
      " Epoch 5\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.5427467909918892\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.5684283342997233\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.5653192042456733\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.5351722846984863\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.6013004995134141\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.5401842186821831\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.5573482804192437\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.6056224847581652\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5742208145565457\n",
      "Test loss for class 0: 0.5211857746124268\n",
      "Test loss for class 1: 0.5515808221817017\n",
      "Test loss for class 2: 0.49866756954193114\n",
      "Test loss for class 3: 0.47708460006713865\n",
      "Test loss for class 4: 0.5159437658309937\n",
      "Test loss for class 5: 0.5021298917770386\n",
      "Test loss for class 6: 0.4933116075515747\n",
      "Test loss for class 7: 0.48020136032104493\n",
      "Test loss for class 8: 0.5550044727325439\n",
      "Overall Accuracy after Epoch 5: 0.10\n",
      " Epoch 6\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.5233134427388509\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.547724828338623\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.5461008258395725\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.517485558573405\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.5807488297780354\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.5243305255042182\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.5338237240261502\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.5829839729732937\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5544850696563721\n",
      "Test loss for class 0: 0.4981020296096802\n",
      "Test loss for class 1: 0.5334699087142944\n",
      "Test loss for class 2: 0.47396667556762695\n",
      "Test loss for class 3: 0.45540026836395264\n",
      "Test loss for class 4: 0.49056497116088865\n",
      "Test loss for class 5: 0.4813709091186523\n",
      "Test loss for class 6: 0.46804552383422854\n",
      "Test loss for class 7: 0.45401012954711917\n",
      "Test loss for class 8: 0.5355100650787353\n",
      "Overall Accuracy after Epoch 6: 0.11\n",
      " Epoch 7\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.5054428859286838\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.5289229189130995\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.5291590497758654\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.5017388665093316\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.5615557720184327\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.5095846446143256\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.5128252345191108\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.5619324718051486\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5364589366488987\n",
      "Test loss for class 0: 0.47639782180786133\n",
      "Test loss for class 1: 0.5152348949432373\n",
      "Test loss for class 2: 0.451467533493042\n",
      "Test loss for class 3: 0.4340327524185181\n",
      "Test loss for class 4: 0.4637268684387207\n",
      "Test loss for class 5: 0.460480633354187\n",
      "Test loss for class 6: 0.44498888301849365\n",
      "Test loss for class 7: 0.43095256156921385\n",
      "Test loss for class 8: 0.5063822048187255\n",
      "Overall Accuracy after Epoch 7: 0.11\n",
      " Epoch 8\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.4891861967722575\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.5113682840559217\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.5132050883399115\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.4873536078135173\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.5432403315650092\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.49602405268351235\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.49335222053527833\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.5417289479573568\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5192664189656575\n",
      "Test loss for class 0: 0.46167754955291745\n",
      "Test loss for class 1: 0.5012009340286255\n",
      "Test loss for class 2: 0.43222529621124267\n",
      "Test loss for class 3: 0.4144926465988159\n",
      "Test loss for class 4: 0.4427206144332886\n",
      "Test loss for class 5: 0.4413233497619629\n",
      "Test loss for class 6: 0.42213566131591795\n",
      "Test loss for class 7: 0.40705422801971436\n",
      "Test loss for class 8: 0.47882342491149904\n",
      "Overall Accuracy after Epoch 8: 0.11\n",
      " Epoch 9\n",
      "-------------------------------\n",
      "Training classifier for Class 0\n",
      "Training loss: 0.47404043880038793\n",
      "Training classifier for Class 1\n",
      "Training loss: 0.49492548857794866\n",
      "Training classifier for Class 2\n",
      "Training loss: 0.49829037111070423\n",
      "Training classifier for Class 3\n",
      "Training loss: 0.4738447325812446\n",
      "Training classifier for Class 4\n",
      "Training loss: 0.5257660005357531\n",
      "Training classifier for Class 5\n",
      "Training loss: 0.48308830661773683\n",
      "Training classifier for Class 6\n",
      "Training loss: 0.47546673397488065\n",
      "Training classifier for Class 7\n",
      "Training loss: 0.5226673091888427\n",
      "Training classifier for Class 8\n",
      "Training loss: 0.5030488472832574\n",
      "Test loss for class 0: 0.44066995735168457\n",
      "Test loss for class 1: 0.4835887840270996\n",
      "Test loss for class 2: 0.41226588554382326\n",
      "Test loss for class 3: 0.3961640619277954\n",
      "Test loss for class 4: 0.4179970073699951\n",
      "Test loss for class 5: 0.4221514711380005\n",
      "Test loss for class 6: 0.40510444145202634\n",
      "Test loss for class 7: 0.3839407829284668\n",
      "Test loss for class 8: 0.4551916681289673\n",
      "Overall Accuracy after Epoch 9: 0.12\n",
      "Final Total accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(9):\n",
    "    model_multi = SVM(32*32*3, num_classes=1).to(device)\n",
    "    models.append(model_multi)\n",
    "\n",
    "total_correct = 0\n",
    "result_tens = None\n",
    "train_tens = None\n",
    "\n",
    "for epoch in range(9):\n",
    "    print(f\" Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    for class_num in range(9):\n",
    "        print(f\"Training classifier for Class {class_num}\")\n",
    "        train_pred, train_correct, train_loss, train_tens = train_loop_Multi_SVM(train_set, models[class_num], gamma=0.01, lr=0.0001, class_num=class_num)\n",
    "    \n",
    "    total_correct, result_tens = test_loop_Multi_SVM(testset, models, train_tens, gamma=0.01)\n",
    "    accuracy = total_correct / len(testset.dataset)\n",
    "    print(f\"Overall Accuracy after Epoch {epoch+1}: {accuracy:.2f}\")\n",
    "\n",
    "print(f\"Final Total accuracy: {total_correct / len(testset.dataset):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 71\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 95\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 112\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 129\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 141\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 143\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 155\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 158\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 160\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 170\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 185\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 209\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 220\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 221\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 233\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 250\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 261\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 262\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 311\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 313\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 329\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 334\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 341\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 344\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 348\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 354\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 392\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 396\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 402\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 407\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 448\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 450\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 473\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 486\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 492\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 493\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 503\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 557\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 571\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 583\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 593\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 641\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 642\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 651\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 664\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 676\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 685\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 689\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 690\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 694\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 697\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 702\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 742\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 750\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 757\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 787\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 822\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 851\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 859\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 869\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 883\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 890\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 898\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 910\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 923\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 926\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 968\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 993\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 1021\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1032\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 1106\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 1134\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1140\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 1168\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1178\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 1193\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1200\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 1207\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1213\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 1240\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1252\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 1254\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1265\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 1277\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1282\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 1339\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1343\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1410\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1436\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 1438\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1461\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1462\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 1529\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 1539\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1542\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 1575\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 1589\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 1595\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 1622\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 1633\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 1637\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 1638\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1669\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1670\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1717\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 1763\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1770\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1781\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1794\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1858\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1883\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 1890\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 1902\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 1908\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 1933\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 1986\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2001\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2011\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2028\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 2041\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 2061\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2074\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2091\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2106\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2117\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2143\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2151\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2162\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2171\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2178\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 2194\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2196\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2223\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2280\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 2283\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2313\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 2331\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2345\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2348\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2352\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 2354\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 2379\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2405\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2413\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2414\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2430\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2451\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2454\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2455\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2472\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2477\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2509\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 2524\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2535\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2558\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2564\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2570\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2573\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2582\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2592\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2594\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2606\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2624\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2629\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 2640\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2644\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2653\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 2663\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2681\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2692\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2699\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 2713\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 2718\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2724\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2730\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2747\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2751\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2756\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2762\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 2769\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2803\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 2825\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 2845\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2862\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2918\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 2924\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 2949\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 2955\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3015\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 3021\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3029\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3041\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3064\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3077\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3101\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 3123\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 3131\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 3166\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 3181\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3196\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 3197\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 3198\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3237\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3249\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3340\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 3375\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3382\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3401\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 3437\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3439\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3448\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 3453\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 3487\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3495\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3545\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 3550\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 3588\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3594\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 3597\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3600\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 3607\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 3658\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 3666\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3685\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3700\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3722\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3725\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3736\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 3787\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3797\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3814\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 3820\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 3838\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3839\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3855\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3903\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3935\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3937\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 3939\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 3944\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 3946\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 3947\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 3948\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 3999\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4000\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4009\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4035\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 4039\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4075\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4080\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4086\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4088\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4090\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4120\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4136\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4156\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4165\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4166\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 4179\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4188\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4208\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4213\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4214\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4230\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4237\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 4256\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4275\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 4278\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 4306\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 4311\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4322\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 4325\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4330\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4343\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4410\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4531\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4553\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4555\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4569\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 4573\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 4594\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 4628\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4638\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4741\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4760\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4773\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4788\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 4793\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4851\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 4859\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4879\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4903\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 4912\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4928\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 4952\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 4966\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 4995\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5014\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5017\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 5028\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5031\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5038\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5050\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5054\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5055\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5075\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5100\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 5105\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5112\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5118\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 5122\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5142\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5150\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5157\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 5160\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5191\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 5193\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5230\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5277\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5297\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5315\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 5329\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5331\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5379\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 5385\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 5404\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5410\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5446\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 5463\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5468\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5516\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5517\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5539\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5541\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5547\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 5593\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 5597\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5600\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5605\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5618\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5643\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5657\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5668\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5669\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 5672\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5684\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 5695\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5709\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5757\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5788\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 5793\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5796\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5813\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5820\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 5838\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5857\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5862\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5863\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 5894\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 5899\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5906\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5940\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 5946\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5948\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5949\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 5971\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 5980\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 5999\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6033\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6036\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6059\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 6062\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6083\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6100\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6120\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6132\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6133\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6139\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 6164\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6165\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6169\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 6236\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 6322\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6326\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6343\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6372\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6375\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6384\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6413\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6421\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6429\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6457\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6490\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6499\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6506\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6510\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6517\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6538\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 6554\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6562\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6592\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6597\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6625\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6627\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 6630\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 6659\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6663\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6670\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 6685\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6712\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 6722\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6724\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6773\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 6788\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 6789\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6790\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 6802\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6884\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6888\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6931\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 6941\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 6946\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 6957\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6958\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 6959\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 6964\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 6995\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7005\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7012\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7044\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 7065\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7191\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 7214\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7251\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7256\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7287\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7296\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7353\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 7373\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7382\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 7389\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 7392\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7405\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7433\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7462\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 7466\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7471\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 7476\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 7484\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 7500\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7539\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 7542\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 7551\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7552\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 7600\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7627\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7631\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7660\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7678\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7692\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 7693\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 7701\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7706\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 7709\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7730\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7756\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 7765\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 7777\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 7794\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 7834\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7841\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 7853\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7865\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 7881\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 7929\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 7970\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 7977\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8028\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8029\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8033\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 8035\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8039\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 8040\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 8048\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8056\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8094\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 8131\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8138\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8193\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8222\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 8229\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8254\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8261\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 8270\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8301\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8308\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 8321\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 8338\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 8347\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8356\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 8384\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8392\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8394\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 8431\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 8442\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 8475\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 8479\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 8480\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8534\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8540\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 8565\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 8574\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 8606\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8612\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 8616\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 8651\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8657\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8678\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 8693\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 8702\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8716\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8778\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 8794\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8800\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 8802\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8831\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 8840\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8895\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8903\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 8941\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 8945\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 8962\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 8993\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 8995\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 8999\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9038\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9088\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9098\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9116\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9124\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9142\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9144\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9169\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9170\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9184\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9185\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9193\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9240\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9243\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 9254\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9257\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9259\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9278\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 9311\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9314\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9326\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 9362\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9363\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9365\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9374\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9401\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9408\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 9437\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9440\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9442\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9453\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9456\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9457\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9467\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 9473\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9515\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 9523\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9529\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 9541\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9549\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9552\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9553\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9555\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9569\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9577\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9607\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 9619\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9636\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9638\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9648\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9653\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9661\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1.,  1., -1.]) 9679\n",
      "tensor([-1., -1.,  1., -1., -1., -1., -1., -1., -1.]) 9709\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 9735\n",
      "tensor([-1.,  1., -1., -1., -1., -1., -1., -1., -1.]) 9740\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 9827\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9833\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9852\n",
      "tensor([-1., -1., -1.,  1., -1., -1., -1., -1., -1.]) 9854\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9860\n",
      "tensor([-1., -1., -1., -1., -1.,  1., -1., -1., -1.]) 9876\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9878\n",
      "tensor([-1., -1., -1., -1.,  1., -1., -1., -1., -1.]) 9883\n",
      "tensor([-1., -1., -1., -1., -1., -1.,  1., -1., -1.]) 9898\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9918\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9927\n",
      "tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  1.]) 9938\n",
      "tensor([ 1., -1., -1., -1., -1., -1., -1., -1., -1.]) 9940\n",
      "Final Total accuracy: 0.06\n"
     ]
    }
   ],
   "source": [
    "# result after running 1 epoch\n",
    "total_correct = 0\n",
    "for i in range(len(result_tens)):\n",
    "  if (result_tens[i] == -1).all() or ((result_tens[i] == 1).sum() == 1 and (result_tens[i] == -1).sum() == len(result_tens[i]) - 1):\n",
    "    if train_tens[i] == 1:\n",
    "      print(result_tens[i], i)\n",
    "      total_correct += 1\n",
    "\n",
    "print(f\"Final Total accuracy: {total_correct / len(testset.dataset):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Perform hyperparamter tuning for the multiclass SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without too much printing out the results\n",
    "\n",
    "# Train the model\n",
    "def train_loop_Multi_SVM(dataloader, model, gamma, lr, class_num):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  train_tens = torch.zeros((len(dataloader.dataset), 1))\n",
    "\n",
    "  optim = optimizer(model, lr)\n",
    "  \n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss, correct, res = loss_hinge_Multi_SVM(pred, y, gamma, model, class_num)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "  \n",
    "    total_loss += loss.item() * X.size(0)\n",
    "    total_correct += correct\n",
    "\n",
    "    for i in range(len(res)):\n",
    "      sample_idx = batch * dataloader.batch_size + i\n",
    "      train_tens[sample_idx] = res[i]\n",
    "    \n",
    "  #print(f\"Training loss: {total_loss / size}\")\n",
    "  #print(f\"Training accuracy: {total_correct / size}\")\n",
    "  return pred, total_correct, total_loss, train_tens\n",
    "\n",
    "# Test the model\n",
    "def test_loop_Multi_SVM(dataloader, models, train_tens, gamma):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_correct = 0\n",
    "  result_tens = torch.zeros((len(dataloader.dataset), 9))  # 2D tensor for storing predictions across classes\n",
    "\n",
    "  for class_num in range(9): \n",
    "    model_ = models[class_num]\n",
    "    model_.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_gt = torch.where(y == class_num, 1, -1).to(device)\n",
    "        \n",
    "        pred = model_(X)\n",
    "        loss, correct, res = loss_hinge_Multi_SVM(pred, y_gt, gamma, model_, class_num)\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        for i in range(len(res)):\n",
    "          sample_idx = batch_idx * dataloader.batch_size + i\n",
    "          result_tens[sample_idx, class_num] = res[i]\n",
    "\n",
    "    #print(f\"Test loss for class {class_num}: {total_loss / size}\")\n",
    "\n",
    "  for i in range(len(result_tens)): \n",
    "    res_ = result_tens[i]\n",
    "    if (res_ == -1).all() or ((res_ == 1).sum() == 1 and (res_ == -1).sum() == len(res_) - 1):\n",
    "      if train_tens[i] == 1:\n",
    "        total_correct += 1\n",
    "\n",
    "  #print(\"Result Tensor:\", result_tens) \n",
    "  return total_correct, result_tens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.001, lr=0.0001\n",
      "Overall Accuracy after Epoch 9: 0.11\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.001, lr=0.0003\n",
      "Overall Accuracy after Epoch 9: 0.02\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.001, lr=0.001\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.001, lr=0.003\n",
      "Overall Accuracy after Epoch 9: 0.01\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.001, lr=0.01\n",
      "Overall Accuracy after Epoch 9: 0.04\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.003, lr=0.0001\n",
      "Overall Accuracy after Epoch 9: 0.10\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.003, lr=0.0003\n",
      "Overall Accuracy after Epoch 9: 0.03\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.003, lr=0.001\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.003, lr=0.003\n",
      "Overall Accuracy after Epoch 9: 0.01\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.003, lr=0.01\n",
      "Overall Accuracy after Epoch 9: 0.05\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.01, lr=0.0001\n",
      "Overall Accuracy after Epoch 9: 0.11\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.01, lr=0.0003\n",
      "Overall Accuracy after Epoch 9: 0.02\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.01, lr=0.001\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.01, lr=0.003\n",
      "Overall Accuracy after Epoch 9: 0.01\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.01, lr=0.01\n",
      "Overall Accuracy after Epoch 9: 0.05\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.03, lr=0.0001\n",
      "Overall Accuracy after Epoch 9: 0.11\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.03, lr=0.0003\n",
      "Overall Accuracy after Epoch 9: 0.02\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.03, lr=0.001\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.03, lr=0.003\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.03, lr=0.01\n",
      "Overall Accuracy after Epoch 9: 0.05\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.1, lr=0.0001\n",
      "Overall Accuracy after Epoch 9: 0.12\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.1, lr=0.0003\n",
      "Overall Accuracy after Epoch 9: 0.01\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.1, lr=0.001\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.1, lr=0.003\n",
      "Overall Accuracy after Epoch 9: 0.00\n",
      "\n",
      "-----------------------------------\n",
      "Testing gamma=0.1, lr=0.01\n",
      "Overall Accuracy after Epoch 9: 0.04\n",
      "(0.119, 0.1, 0.0001)\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "result_tens = None\n",
    "train_tens = None\n",
    "accuracy = 0\n",
    "\n",
    "gamma_values = [0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "lr_values = [0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
    "\n",
    "result = []\n",
    "\n",
    "for gamma in gamma_values:\n",
    "  for lr in lr_values:\n",
    "    print(f\"\\n-----------------------------------\")\n",
    "    print(f\"Testing gamma={gamma}, lr={lr}\")\n",
    "    \n",
    "    models = []\n",
    "    for i in range(9):\n",
    "      model_multi = SVM(32*32*3, num_classes=1).to(device)\n",
    "      models.append(model_multi)\n",
    "    \n",
    "    for epoch in range(9):\n",
    "      \n",
    "      \n",
    "      for class_num in range(9):\n",
    "        #print(f\"Training classifier for Class {class_num}\")\n",
    "        train_pred, train_correct, train_loss, train_tens = train_loop_Multi_SVM(train_set, models[class_num], gamma=gamma, lr=lr, class_num=class_num)\n",
    "      \n",
    "      total_correct, result_tens = test_loop_Multi_SVM(val_set, models, train_tens, gamma=gamma)\n",
    "      if epoch == 8:\n",
    "        accuracy = total_correct / len(val_set.dataset)\n",
    "        print(f\"Overall Accuracy after Epoch {epoch+1}: {accuracy:.2f}\")\n",
    "      \n",
    "    result.append((accuracy, gamma, lr))\n",
    "\n",
    "result = sorted(result, key=lambda x: (x[0], x[1], x[2]), reverse=True)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy after Epoch 1: 0.09\n",
      "Overall Accuracy after Epoch 2: 0.08\n",
      "Overall Accuracy after Epoch 3: 0.10\n",
      "Overall Accuracy after Epoch 4: 0.10\n",
      "Overall Accuracy after Epoch 5: 0.10\n",
      "Overall Accuracy after Epoch 6: 0.11\n",
      "Overall Accuracy after Epoch 7: 0.11\n",
      "Overall Accuracy after Epoch 8: 0.12\n",
      "Overall Accuracy after Epoch 9: 0.12\n",
      "Final Total accuracy: 0.12\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(9):\n",
    "    model_multi = SVM(32*32*3, num_classes=1).to(device)\n",
    "    models.append(model_multi)\n",
    "\n",
    "total_correct = 0\n",
    "result_tens = None\n",
    "train_tens = None\n",
    "\n",
    "for epoch in range(9):\n",
    "    #print(f\" Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    for class_num in range(9):\n",
    "        #print(f\"Training classifier for Class {class_num}\")\n",
    "        train_pred, train_correct, train_loss, train_tens = train_loop_Multi_SVM(train_set, models[class_num], gamma=0.1, lr=0.0001, class_num=class_num)\n",
    "    \n",
    "    total_correct, result_tens = test_loop_Multi_SVM(testset, models, train_tens, gamma=0.01)\n",
    "    accuracy = total_correct / len(testset.dataset)\n",
    "    print(f\"Overall Accuracy after Epoch {epoch+1}: {accuracy:.2f}\")\n",
    "\n",
    "print(f\"Final Total accuracy: {total_correct / len(testset.dataset):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Implement your own multinomial / multi class logistic regression from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train set size:  45000\n",
      "Validation set size:  5000\n",
      "Test set size:  10000\n"
     ]
    }
   ],
   "source": [
    "# Re-load the dataset and Normalize\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_set, val_set = split_train_val(trainset)\n",
    "print(\"Train set size: \", len(train_set))\n",
    "print(\"Validation set size: \", len(val_set))\n",
    "print(\"Test set size: \", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_set = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "testset = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Reg()\n"
     ]
    }
   ],
   "source": [
    "class Logistic_Reg(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super(Logistic_Reg, self).__init__()\n",
    "    self.weights = torch.randn(input_dim, 10, requires_grad=True)*0.0001\n",
    "    self.bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = x @ self.weights + self.bias\n",
    "    return x\n",
    "Logistic_model = Logistic_Reg(32*32*3)\n",
    "Logistic_model = Logistic_model.to(device)\n",
    "print(Logistic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from torch.optim import Adam\n",
    "def optimizer_logistic(Logistic_model, lr):\n",
    "    Logistic_model.weights = nn.Parameter(Logistic_model.weights)\n",
    "    Logistic_model.bias = nn.Parameter(Logistic_model.bias)\n",
    "    return Adam(Logistic_model.parameters(), lr)\n",
    "\n",
    "optim_logistic = optimizer_logistic(Logistic_model, lr=0.0001)\n",
    "\n",
    "def CrossEntropy_Loss(y_pred, y_true, Logistic_model):\n",
    "    y_true_converge = torch.zeros((len(y_true), 10))\n",
    "    for i in range(len(y_true)):\n",
    "      y_true_converge[i, y_true[i]] = 1\n",
    "\n",
    "    exp = torch.exp(y_pred)\n",
    "    sum = torch.sum(exp, dim=1, keepdim=True) \n",
    "    softmax = exp/sum\n",
    "    log_softmax = torch.log(softmax)\n",
    "    \n",
    "    gt_label_softmax = torch.zeros(y_true.size())\n",
    "    for i in range(len(y_true)):\n",
    "        gt_label_softmax[i] = log_softmax[i, y_true[i]]\n",
    "    \n",
    "    loss = -(gt_label_softmax.mean() * y_true_converge.mean())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "def train_loop_logistic(dataloader, Logistic_model, lr):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  \n",
    "  # Define the optimizer\n",
    "  optim = optimizer_logistic(Logistic_model, lr)\n",
    "    \n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    pred = Logistic_model(X)\n",
    "    loss = CrossEntropy_Loss(pred, y, Logistic_model)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    total_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted = torch.argmax(pred, dim=1)\n",
    "    total_correct += (predicted == y).sum().item()\n",
    "        \n",
    "  print(f\"Training loss: {total_loss / size}\")\n",
    "  print(f\"Training accuracy: {total_correct / size}\")\n",
    "  return total_correct / size\n",
    "\n",
    "# Test the model\n",
    "def test_loop_logistic(dataloader, Logistic_model):\n",
    "  size = len(dataloader.dataset)\n",
    "  Logistic_model.eval()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      \n",
    "      pred = Logistic_model(X)\n",
    "      loss = CrossEntropy_Loss(pred, y, Logistic_model)\n",
    "\n",
    "      total_loss += loss.item() * X.size(0)\n",
    "      \n",
    "      # Calculate accuracy\n",
    "      predicted = torch.argmax(pred, dim=1)\n",
    "      total_correct += (predicted == y).sum().item()\n",
    "      \n",
    "  print(f\"Test loss: {total_loss / size}\")\n",
    "  print(f\"Test accuracy: {total_correct / size}\")\n",
    "  return total_loss / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Perform hyperparameter tuning for the multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.8535472593095568\n",
      "Training accuracy: 0.36233333333333334\n",
      "Test loss: 1.7778145320892333\n",
      "Test accuracy: 0.3958\n",
      "Epoch 2, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.7720880311754015\n",
      "Training accuracy: 0.3964666666666667\n",
      "Test loss: 1.744803544998169\n",
      "Test accuracy: 0.4118\n",
      "Epoch 3, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.7451235362794664\n",
      "Training accuracy: 0.4061111111111111\n",
      "Test loss: 1.7461146120071411\n",
      "Test accuracy: 0.4144\n",
      "Epoch 4, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.7273397235446506\n",
      "Training accuracy: 0.41357777777777777\n",
      "Test loss: 1.7411558589935303\n",
      "Test accuracy: 0.4092\n",
      "Epoch 5, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.7180433121999106\n",
      "Training accuracy: 0.41533333333333333\n",
      "Test loss: 1.7278964265823364\n",
      "Test accuracy: 0.4066\n",
      "Epoch 6, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.7057093061023287\n",
      "Training accuracy: 0.42106666666666664\n",
      "Test loss: 1.7255683406829834\n",
      "Test accuracy: 0.4126\n",
      "Epoch 7, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.6971346535152858\n",
      "Training accuracy: 0.4231333333333333\n",
      "Test loss: 1.717389102745056\n",
      "Test accuracy: 0.4178\n",
      "Epoch 8, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.6916954324934217\n",
      "Training accuracy: 0.4272666666666667\n",
      "Test loss: 1.711140168762207\n",
      "Test accuracy: 0.422\n",
      "Epoch 9, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.6861158851835463\n",
      "Training accuracy: 0.4272666666666667\n",
      "Test loss: 1.7153562170982362\n",
      "Test accuracy: 0.4138\n",
      "Epoch 10, lr: 0.0001\n",
      "-------------------------------\n",
      "Training loss: 1.6798080350875855\n",
      "Training accuracy: 0.4272666666666667\n",
      "Test loss: 1.7251936466217042\n",
      "Test accuracy: 0.415\n",
      "Epoch 1, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.8419111171510485\n",
      "Training accuracy: 0.36364444444444444\n",
      "Test loss: 1.771395414352417\n",
      "Test accuracy: 0.398\n",
      "Epoch 2, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.767644206407335\n",
      "Training accuracy: 0.39564444444444447\n",
      "Test loss: 1.759113021850586\n",
      "Test accuracy: 0.3982\n",
      "Epoch 3, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.7407177737341988\n",
      "Training accuracy: 0.4054888888888889\n",
      "Test loss: 1.7348463409423829\n",
      "Test accuracy: 0.4046\n",
      "Epoch 4, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.725132178624471\n",
      "Training accuracy: 0.4108888888888889\n",
      "Test loss: 1.743683236694336\n",
      "Test accuracy: 0.404\n",
      "Epoch 5, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.713866695743137\n",
      "Training accuracy: 0.41357777777777777\n",
      "Test loss: 1.733383145904541\n",
      "Test accuracy: 0.4094\n",
      "Epoch 6, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.7020477707544963\n",
      "Training accuracy: 0.41833333333333333\n",
      "Test loss: 1.7260168243408203\n",
      "Test accuracy: 0.4088\n",
      "Epoch 7, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.694816411972046\n",
      "Training accuracy: 0.42173333333333335\n",
      "Test loss: 1.721142197227478\n",
      "Test accuracy: 0.4132\n",
      "Epoch 8, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.6887607332865398\n",
      "Training accuracy: 0.4265111111111111\n",
      "Test loss: 1.734529962158203\n",
      "Test accuracy: 0.4028\n",
      "Epoch 9, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.6844099002202353\n",
      "Training accuracy: 0.4267111111111111\n",
      "Test loss: 1.747224732208252\n",
      "Test accuracy: 0.3986\n",
      "Epoch 10, lr: 0.0002\n",
      "-------------------------------\n",
      "Training loss: 1.6786977672576904\n",
      "Training accuracy: 0.42948888888888886\n",
      "Test loss: 1.7163420715332032\n",
      "Test accuracy: 0.4158\n",
      "Epoch 1, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.8461928580178155\n",
      "Training accuracy: 0.36286666666666667\n",
      "Test loss: 1.8292537956237793\n",
      "Test accuracy: 0.36\n",
      "Epoch 2, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7756612429936727\n",
      "Training accuracy: 0.3904444444444444\n",
      "Test loss: 1.7738065919876098\n",
      "Test accuracy: 0.3946\n",
      "Epoch 3, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7478609338760376\n",
      "Training accuracy: 0.401\n",
      "Test loss: 1.7852507820129395\n",
      "Test accuracy: 0.3794\n",
      "Epoch 4, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7341629778967964\n",
      "Training accuracy: 0.4076666666666667\n",
      "Test loss: 1.751232349205017\n",
      "Test accuracy: 0.4068\n",
      "Epoch 5, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7226019369337293\n",
      "Training accuracy: 0.4129111111111111\n",
      "Test loss: 1.7570304183959962\n",
      "Test accuracy: 0.399\n",
      "Epoch 6, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7147069475597805\n",
      "Training accuracy: 0.4146\n",
      "Test loss: 1.7552518295288086\n",
      "Test accuracy: 0.4062\n",
      "Epoch 7, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.7066212310579087\n",
      "Training accuracy: 0.414\n",
      "Test loss: 1.7366867486953734\n",
      "Test accuracy: 0.4044\n",
      "Epoch 8, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.699146165614658\n",
      "Training accuracy: 0.418\n",
      "Test loss: 1.7400749931335449\n",
      "Test accuracy: 0.4014\n",
      "Epoch 9, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.693690978749593\n",
      "Training accuracy: 0.42004444444444444\n",
      "Test loss: 1.7681204612731933\n",
      "Test accuracy: 0.3868\n",
      "Epoch 10, lr: 0.0003\n",
      "-------------------------------\n",
      "Training loss: 1.6886633927663168\n",
      "Training accuracy: 0.42473333333333335\n",
      "Test loss: 1.7535744499206543\n",
      "Test accuracy: 0.4052\n",
      "Epoch 1, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.861541386879815\n",
      "Training accuracy: 0.36002222222222224\n",
      "Test loss: 1.7902123050689698\n",
      "Test accuracy: 0.3874\n",
      "Epoch 2, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7948261559804282\n",
      "Training accuracy: 0.38693333333333335\n",
      "Test loss: 1.8260934062957763\n",
      "Test accuracy: 0.3802\n",
      "Epoch 3, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7673111755794948\n",
      "Training accuracy: 0.3947333333333333\n",
      "Test loss: 1.7703900047302246\n",
      "Test accuracy: 0.3984\n",
      "Epoch 4, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7489521825154621\n",
      "Training accuracy: 0.40328888888888886\n",
      "Test loss: 1.7761123519897462\n",
      "Test accuracy: 0.3978\n",
      "Epoch 5, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7386666538450453\n",
      "Training accuracy: 0.4064\n",
      "Test loss: 1.8011711135864257\n",
      "Test accuracy: 0.396\n",
      "Epoch 6, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7284909432517157\n",
      "Training accuracy: 0.40826666666666667\n",
      "Test loss: 1.78031434135437\n",
      "Test accuracy: 0.3906\n",
      "Epoch 7, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7216685309092203\n",
      "Training accuracy: 0.4132222222222222\n",
      "Test loss: 1.80382716217041\n",
      "Test accuracy: 0.391\n",
      "Epoch 8, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7191868023024666\n",
      "Training accuracy: 0.4099111111111111\n",
      "Test loss: 1.760221559524536\n",
      "Test accuracy: 0.405\n",
      "Epoch 9, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7116677096472845\n",
      "Training accuracy: 0.41542222222222225\n",
      "Test loss: 1.8003540660858155\n",
      "Test accuracy: 0.385\n",
      "Epoch 10, lr: 0.0004\n",
      "-------------------------------\n",
      "Training loss: 1.7082475409613715\n",
      "Training accuracy: 0.41835555555555554\n",
      "Test loss: 1.762904658126831\n",
      "Test accuracy: 0.3996\n",
      "Epoch 1, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.87582718717787\n",
      "Training accuracy: 0.3560888888888889\n",
      "Test loss: 1.8081933591842652\n",
      "Test accuracy: 0.3792\n",
      "Epoch 2, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.8085708638509115\n",
      "Training accuracy: 0.38275555555555557\n",
      "Test loss: 1.7938763841629028\n",
      "Test accuracy: 0.3734\n",
      "Epoch 3, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.7865755521350437\n",
      "Training accuracy: 0.38897777777777776\n",
      "Test loss: 1.7795298141479492\n",
      "Test accuracy: 0.3924\n",
      "Epoch 4, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.766184358130561\n",
      "Training accuracy: 0.3976222222222222\n",
      "Test loss: 1.7849139171600341\n",
      "Test accuracy: 0.3828\n",
      "Epoch 5, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.7561686979081896\n",
      "Training accuracy: 0.39964444444444447\n",
      "Test loss: 1.81143780670166\n",
      "Test accuracy: 0.3948\n",
      "Epoch 6, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.7497940571255155\n",
      "Training accuracy: 0.4033111111111111\n",
      "Test loss: 1.7651102886199952\n",
      "Test accuracy: 0.3972\n",
      "Epoch 7, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.742086289299859\n",
      "Training accuracy: 0.40546666666666664\n",
      "Test loss: 1.811864821624756\n",
      "Test accuracy: 0.394\n",
      "Epoch 8, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.737810131963094\n",
      "Training accuracy: 0.4050888888888889\n",
      "Test loss: 1.779050535583496\n",
      "Test accuracy: 0.3946\n",
      "Epoch 9, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.7290825115203858\n",
      "Training accuracy: 0.4099111111111111\n",
      "Test loss: 1.8156542572021483\n",
      "Test accuracy: 0.3738\n",
      "Epoch 10, lr: 0.0005\n",
      "-------------------------------\n",
      "Training loss: 1.7296305046081544\n",
      "Training accuracy: 0.4098888888888889\n",
      "Test loss: 1.8739870832443237\n",
      "Test accuracy: 0.3762\n",
      "Epoch 1, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.895848459646437\n",
      "Training accuracy: 0.3541111111111111\n",
      "Test loss: 1.8269947956085204\n",
      "Test accuracy: 0.3746\n",
      "Epoch 2, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.8293110319349501\n",
      "Training accuracy: 0.37895555555555555\n",
      "Test loss: 1.844964898300171\n",
      "Test accuracy: 0.3814\n",
      "Epoch 3, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.805109138806661\n",
      "Training accuracy: 0.387\n",
      "Test loss: 1.8549254837036133\n",
      "Test accuracy: 0.3706\n",
      "Epoch 4, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.7845667340596516\n",
      "Training accuracy: 0.3922\n",
      "Test loss: 1.8458275665283204\n",
      "Test accuracy: 0.3666\n",
      "Epoch 5, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.778395496368408\n",
      "Training accuracy: 0.3957333333333333\n",
      "Test loss: 1.8661089080810547\n",
      "Test accuracy: 0.3664\n",
      "Epoch 6, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.765095773018731\n",
      "Training accuracy: 0.40013333333333334\n",
      "Test loss: 1.7973409706115722\n",
      "Test accuracy: 0.3868\n",
      "Epoch 7, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.7619335527631972\n",
      "Training accuracy: 0.3988\n",
      "Test loss: 1.8182922384262086\n",
      "Test accuracy: 0.368\n",
      "Epoch 8, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.764582054477268\n",
      "Training accuracy: 0.4010666666666667\n",
      "Test loss: 1.8294750720977784\n",
      "Test accuracy: 0.3692\n",
      "Epoch 9, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.7501775440216065\n",
      "Training accuracy: 0.4050888888888889\n",
      "Test loss: 1.8303218852996825\n",
      "Test accuracy: 0.3844\n",
      "Epoch 10, lr: 0.0006\n",
      "-------------------------------\n",
      "Training loss: 1.748616467920939\n",
      "Training accuracy: 0.4038888888888889\n",
      "Test loss: 1.8344325492858886\n",
      "Test accuracy: 0.394\n",
      "Epoch 1, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.9163688935597738\n",
      "Training accuracy: 0.3443111111111111\n",
      "Test loss: 1.8455698255538941\n",
      "Test accuracy: 0.3718\n",
      "Epoch 2, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.8486180615531074\n",
      "Training accuracy: 0.37324444444444443\n",
      "Test loss: 1.8488499408721923\n",
      "Test accuracy: 0.365\n",
      "Epoch 3, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.8228619454489814\n",
      "Training accuracy: 0.3824444444444444\n",
      "Test loss: 1.851155534362793\n",
      "Test accuracy: 0.372\n",
      "Epoch 4, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.8086971321529812\n",
      "Training accuracy: 0.38764444444444446\n",
      "Test loss: 1.8214764032363893\n",
      "Test accuracy: 0.3712\n",
      "Epoch 5, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.800888621436225\n",
      "Training accuracy: 0.3907555555555556\n",
      "Test loss: 1.8579100996017457\n",
      "Test accuracy: 0.3758\n",
      "Epoch 6, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.7871212942335342\n",
      "Training accuracy: 0.3952888888888889\n",
      "Test loss: 1.9023658041000366\n",
      "Test accuracy: 0.3716\n",
      "Epoch 7, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.781053175354004\n",
      "Training accuracy: 0.39753333333333335\n",
      "Test loss: 1.8976573320388794\n",
      "Test accuracy: 0.3784\n",
      "Epoch 8, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.7816272075865003\n",
      "Training accuracy: 0.39713333333333334\n",
      "Test loss: 1.830809469604492\n",
      "Test accuracy: 0.386\n",
      "Epoch 9, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.775557493570116\n",
      "Training accuracy: 0.40024444444444446\n",
      "Test loss: 1.8526767358779908\n",
      "Test accuracy: 0.3722\n",
      "Epoch 10, lr: 0.0007\n",
      "-------------------------------\n",
      "Training loss: 1.7677858201344807\n",
      "Training accuracy: 0.4045111111111111\n",
      "Test loss: 1.929564362335205\n",
      "Test accuracy: 0.3602\n",
      "Epoch 1, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.9427952190823026\n",
      "Training accuracy: 0.3436666666666667\n",
      "Test loss: 1.8731774551391602\n",
      "Test accuracy: 0.3738\n",
      "Epoch 2, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8743943554348417\n",
      "Training accuracy: 0.36835555555555555\n",
      "Test loss: 1.9131845455169678\n",
      "Test accuracy: 0.358\n",
      "Epoch 3, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8446023742675781\n",
      "Training accuracy: 0.38035555555555556\n",
      "Test loss: 1.855871658706665\n",
      "Test accuracy: 0.3598\n",
      "Epoch 4, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8342664015452068\n",
      "Training accuracy: 0.383\n",
      "Test loss: 1.8662755996704101\n",
      "Test accuracy: 0.3686\n",
      "Epoch 5, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8146346281687418\n",
      "Training accuracy: 0.38766666666666666\n",
      "Test loss: 1.9323640367507935\n",
      "Test accuracy: 0.3494\n",
      "Epoch 6, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8109687474568685\n",
      "Training accuracy: 0.38997777777777776\n",
      "Test loss: 1.8509347013473512\n",
      "Test accuracy: 0.376\n",
      "Epoch 7, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.8022217129177518\n",
      "Training accuracy: 0.39486666666666664\n",
      "Test loss: 1.9108637626647949\n",
      "Test accuracy: 0.3588\n",
      "Epoch 8, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.798606481107076\n",
      "Training accuracy: 0.39464444444444446\n",
      "Test loss: 1.8934995174407958\n",
      "Test accuracy: 0.3752\n",
      "Epoch 9, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.7999052177005344\n",
      "Training accuracy: 0.393\n",
      "Test loss: 1.9077706336975098\n",
      "Test accuracy: 0.3678\n",
      "Epoch 10, lr: 0.0008\n",
      "-------------------------------\n",
      "Training loss: 1.792544357003106\n",
      "Training accuracy: 0.3963111111111111\n",
      "Test loss: 2.0011804275512697\n",
      "Test accuracy: 0.3566\n",
      "Epoch 1, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.9643764803568522\n",
      "Training accuracy: 0.3379111111111111\n",
      "Test loss: 1.9259742225646972\n",
      "Test accuracy: 0.3506\n",
      "Epoch 2, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8926714444902208\n",
      "Training accuracy: 0.36466666666666664\n",
      "Test loss: 1.89097216796875\n",
      "Test accuracy: 0.3652\n",
      "Epoch 3, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8723508479648165\n",
      "Training accuracy: 0.3732\n",
      "Test loss: 1.896117699432373\n",
      "Test accuracy: 0.3696\n",
      "Epoch 4, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8563048821979098\n",
      "Training accuracy: 0.37908888888888886\n",
      "Test loss: 1.882885308456421\n",
      "Test accuracy: 0.3614\n",
      "Epoch 5, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.848799892891778\n",
      "Training accuracy: 0.3818888888888889\n",
      "Test loss: 1.898792213821411\n",
      "Test accuracy: 0.3734\n",
      "Epoch 6, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.837816561020745\n",
      "Training accuracy: 0.387\n",
      "Test loss: 1.8825782051086426\n",
      "Test accuracy: 0.3642\n",
      "Epoch 7, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8329204934226142\n",
      "Training accuracy: 0.3847555555555556\n",
      "Test loss: 1.8718914720535278\n",
      "Test accuracy: 0.376\n",
      "Epoch 8, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8299463800642226\n",
      "Training accuracy: 0.38795555555555555\n",
      "Test loss: 1.9167299795150756\n",
      "Test accuracy: 0.3744\n",
      "Epoch 9, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.820352705891927\n",
      "Training accuracy: 0.3904222222222222\n",
      "Test loss: 1.9301228385925293\n",
      "Test accuracy: 0.35\n",
      "Epoch 10, lr: 0.0009\n",
      "-------------------------------\n",
      "Training loss: 1.8123635379579333\n",
      "Training accuracy: 0.39395555555555556\n",
      "Test loss: 1.8989862682342529\n",
      "Test accuracy: 0.3718\n"
     ]
    }
   ],
   "source": [
    "# perform hyperparameter tuning\n",
    "# using validation set\n",
    "result_li = []\n",
    "\n",
    "for lr in range(1, 10, 1):\n",
    "  Logistic_model = Logistic_Reg(32*32*3)\n",
    "  Logistic_model = Logistic_model.to(device)\n",
    "  for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1}, lr: {lr/10000}\\n-------------------------------\")\n",
    "    train_acc = train_loop_logistic(train_set, Logistic_model, lr=lr/10000)\n",
    "    val_acc = test_loop_logistic(val_set, Logistic_model)\n",
    "    if epoch == 9:\n",
    "      result_li.append((train_acc, val_acc, lr/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3963111111111111, 2.0011804275512697, 0.0008)\n"
     ]
    }
   ],
   "source": [
    "# optimal hyperparameter\n",
    "result_li = sorted(result_li, key=lambda x: (x[1], x[0]))\n",
    "print(result_li[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) What is the Final test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1.9396388982137045\n",
      "Training accuracy: 0.3444888888888889\n",
      "Test loss: 1.861122340965271\n",
      "Test accuracy: 0.3748\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 1.8676819272359213\n",
      "Training accuracy: 0.3696\n",
      "Test loss: 1.9177850955963134\n",
      "Test accuracy: 0.361\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training loss: 1.8443275566101074\n",
      "Training accuracy: 0.3776\n",
      "Test loss: 1.9133680473327637\n",
      "Test accuracy: 0.3564\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training loss: 1.8311632316589355\n",
      "Training accuracy: 0.38237777777777776\n",
      "Test loss: 1.9192345781326294\n",
      "Test accuracy: 0.3539\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training loss: 1.8199117468092176\n",
      "Training accuracy: 0.3844222222222222\n",
      "Test loss: 1.8916705570220946\n",
      "Test accuracy: 0.3592\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training loss: 1.8142749859703913\n",
      "Training accuracy: 0.3900222222222222\n",
      "Test loss: 1.9053936965942382\n",
      "Test accuracy: 0.3608\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training loss: 1.8046383655971951\n",
      "Training accuracy: 0.3918\n",
      "Test loss: 1.8994418487548828\n",
      "Test accuracy: 0.3628\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training loss: 1.7983146189371744\n",
      "Training accuracy: 0.3968\n",
      "Test loss: 1.9416993381500245\n",
      "Test accuracy: 0.3464\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training loss: 1.8011804955800375\n",
      "Training accuracy: 0.39382222222222224\n",
      "Test loss: 1.92091199760437\n",
      "Test accuracy: 0.3603\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training loss: 1.786489605670505\n",
      "Training accuracy: 0.40095555555555557\n",
      "Test loss: 1.9420865184783935\n",
      "Test accuracy: 0.3456\n"
     ]
    }
   ],
   "source": [
    "# final test accuracy\n",
    "Logistic_model = Logistic_Reg(32*32*3).to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "  print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "  train_acc = train_loop_logistic(train_set, Logistic_model, lr=0.0008)\n",
    "  test_acc = test_loop_logistic(testset, Logistic_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Compare against the model implemented using functions from PyTorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Reg_torch(\n",
      "  (layer1): Linear(in_features=3072, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Logistic_Reg_torch(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super(Logistic_Reg_torch, self).__init__()\n",
    "    self.layer1 = nn.Linear(input_dim, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, start_dim=1)\n",
    "    x = self.layer1(x)\n",
    "    return x\n",
    "  \n",
    "Logistic_model_torch = Logistic_Reg_torch(32*32*3)\n",
    "Logistic_model_torch = Logistic_model_torch.to(device)\n",
    "print(Logistic_model_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from torch.optim import Adam\n",
    "def optimizer_logistic(model, lr):\n",
    "    return Adam(model.parameters(), lr)\n",
    "\n",
    "optim_logistic = optimizer_logistic(Logistic_model_torch, lr=0.0001)\n",
    "\n",
    "def CrossEntropy_Loss_nn(y_pred, y_true):\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    \n",
    "    loss_ = cross_entropy(y_pred, y_true)\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "def train_loop_logistic(dataloader, Logistic_model, lr):\n",
    "  size = len(dataloader.dataset)\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "  \n",
    "  # Define the optimizer\n",
    "  optim = optimizer_logistic(Logistic_model, lr)\n",
    "    \n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "        \n",
    "    pred = Logistic_model(X)\n",
    "    loss = CrossEntropy_Loss_nn(pred, y)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    total_loss += loss.item() * X.size(0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted = torch.argmax(pred, dim=1)\n",
    "    total_correct += (predicted == y).sum().item()\n",
    "        \n",
    "  print(f\"Training loss: {total_loss / size}\")\n",
    "  print(f\"Training accuracy: {total_correct / size}\")\n",
    "  return total_correct / size\n",
    "\n",
    "# Test the model\n",
    "def test_loop_logistic(dataloader, Logistic_model):\n",
    "  size = len(dataloader.dataset)\n",
    "  Logistic_model.eval()\n",
    "  total_loss = 0\n",
    "  total_correct = 0\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      \n",
    "      pred = Logistic_model(X)\n",
    "      loss = CrossEntropy_Loss_nn(pred, y)\n",
    "\n",
    "      total_loss += loss.item() * X.size(0)\n",
    "      \n",
    "      # Calculate accuracy\n",
    "      predicted = torch.argmax(pred, dim=1)\n",
    "      total_correct += (predicted == y).sum().item()\n",
    "      \n",
    "  print(f\"Test loss: {total_loss / size}\")\n",
    "  print(f\"Test accuracy: {total_correct / size}\")\n",
    "  return total_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9419910084194607\n",
      "Training accuracy: 0.3423777777777778\n",
      "Test loss: 1.8593612155914307\n",
      "Test accuracy: 0.3619\n",
      "Training loss: 1.8765240019480387\n",
      "Training accuracy: 0.36715555555555557\n",
      "Test loss: 1.8494980865478516\n",
      "Test accuracy: 0.3693\n",
      "Training loss: 1.8457430002848307\n",
      "Training accuracy: 0.37822222222222224\n",
      "Test loss: 1.8937924064636231\n",
      "Test accuracy: 0.3593\n",
      "Training loss: 1.8290804231855604\n",
      "Training accuracy: 0.3838\n",
      "Test loss: 1.951517528152466\n",
      "Test accuracy: 0.3575\n",
      "Training loss: 1.822738539144728\n",
      "Training accuracy: 0.3852888888888889\n",
      "Test loss: 1.8864539249420167\n",
      "Test accuracy: 0.363\n",
      "Training loss: 1.8122870926327175\n",
      "Training accuracy: 0.3872\n",
      "Test loss: 1.8844112409591676\n",
      "Test accuracy: 0.3631\n",
      "Training loss: 1.8054323323991563\n",
      "Training accuracy: 0.39393333333333336\n",
      "Test loss: 1.8806983116149902\n",
      "Test accuracy: 0.3614\n",
      "Training loss: 1.8057830153571235\n",
      "Training accuracy: 0.39435555555555557\n",
      "Test loss: 1.9091109558105468\n",
      "Test accuracy: 0.3684\n",
      "Training loss: 1.8020669274224175\n",
      "Training accuracy: 0.3932222222222222\n",
      "Test loss: 1.9063685977935791\n",
      "Test accuracy: 0.3573\n",
      "Training loss: 1.7904591535144383\n",
      "Training accuracy: 0.3980666666666667\n",
      "Test loss: 1.9540391613006591\n",
      "Test accuracy: 0.3641\n"
     ]
    }
   ],
   "source": [
    "# train for 10 epochs\n",
    "for epoch in range(10):\n",
    "  train_acc = train_loop_logistic(train_set, Logistic_model_torch, lr=0.0008)\n",
    "  test_acc = test_loop_logistic(testset, Logistic_model_torch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
